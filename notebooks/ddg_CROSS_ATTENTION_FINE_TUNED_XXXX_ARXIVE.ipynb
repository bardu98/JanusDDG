{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d0d193-11f5-4e42-b76f-99c49e887da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "#install required packages\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "import torch_geometric.utils as pyg_utils\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool,GATv2Conv\n",
    "from torch_geometric.nn.models import GCN, GAT\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils import softmax\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import random\n",
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b180d05-7ebd-4b89-8213-2ee2a88debc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008404760555780284"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([0.7280, 0.7030, 0.7110, 0.715,0.72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4522ad10-397a-4b36-abe8-3c215b897876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)  # Python random\n",
    "    np.random.seed(seed)  # Numpy random\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU (un singolo dispositivo)\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch GPU (tutti i dispositivi, se usi multi-GPU)\n",
    "    torch.backends.cudnn.deterministic = True  # Comportamento deterministico di cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Evita che cuDNN ottimizzi dinamicamente (influisce su riproducibilità)\n",
    "\n",
    "# Imposta il seed\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e5244a-fe04-49c4-8766-18daa9de6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm\n",
    "\n",
    "# Carica il modello pre-addestrato ESM2\n",
    "model_esm, alphabet_esm = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "#esm.pretrained.esm2_t48_15B_UR50D() #esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "# #modelli possibili \n",
    "# esm.pretrained.esm2_t6_8M_UR50D()   # Modello più piccolo (8 milioni di parametri)\n",
    "# esm.pretrained.esm2_t33_650M_UR50D() # Modello medio (650 milioni di parametri)\n",
    "# esm.pretrained.esm2_t36_3B_UR50D()   # Modello più grande (3 miliardi di parametri)\n",
    "\n",
    "# Sposta il modello su GPU, se disponibile\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_esm = model_esm.to(device)\n",
    "batch_converter_esm = alphabet_esm.get_batch_converter()\n",
    "model_esm.eval()\n",
    "\n",
    "def Esm2_embedding(seq, model_esm = model_esm, batch_converter_esm = batch_converter_esm):\n",
    "    # Definisci la sequenza della proteina\n",
    "    sequences = [(\"protein\", seq),]\n",
    "    \n",
    "    # Converte la sequenza nel formato richiesto dal modello\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter_esm(sequences)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    #batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    # Disabilita il calcolo del gradiente per risparmiare memoria\n",
    "    with torch.no_grad():\n",
    "        results = model_esm(batch_tokens, repr_layers=[33])  # Usa l'ultimo layer\n",
    "        token_representations = results[\"representations\"][33]\n",
    "    \n",
    "    # Rimuove i token speciali di inizio/fine sequenza\n",
    "    # L'embedding risultante sarà una matrice (Lunghezza della sequenza, Dimensione dell'embedding)\n",
    "    embedding = token_representations[0, 1:-1].cpu().numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd53fc53-6a66-4f06-aa44-c6e5ac8c52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "class DeltaDataset(Dataset):\n",
    "    def __init__(self, data, dim_embedding, inv = False):\n",
    "        self.data = data\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.inv = inv\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        if self.inv: \n",
    "            return {\n",
    "                'id': sample['id'],\n",
    "                'wild_type': torch.tensor(sample['mut_type'], dtype=torch.float32),    # inverto mut con wild \n",
    "                'mut_type': torch.tensor(sample['wild_type'], dtype=torch.float32),    # inverto mut con wild             \n",
    "                'length': torch.tensor(sample['length'], dtype=torch.float32),\n",
    "                'ddg': torch.tensor(-float(sample['ddg']), dtype=torch.float32),       # -ddg\n",
    "                'pos_mut': torch.tensor(sample['pos_mut'], dtype=torch.int64),\n",
    "                #'hydra_slim': torch.tensor(Esm2_embedding(''.join(['X' for n in range(sample['length'])])), dtype=torch.float32),\n",
    "                'hydra_slim': torch.tensor(sample['mut_type']*0, dtype=torch.float32),\n",
    "\n",
    "                }\n",
    "\n",
    "        else:\n",
    "            return {\n",
    "                'id': sample['id'],\n",
    "                'wild_type': torch.tensor(sample['wild_type'], dtype=torch.float32),\n",
    "                'mut_type': torch.tensor(sample['mut_type'],dtype=torch.float32),\n",
    "                'length': torch.tensor(sample['length'], dtype=torch.float32),\n",
    "                'ddg': torch.tensor(float(sample['ddg']), dtype=torch.float32),\n",
    "                'pos_mut': torch.tensor(sample['pos_mut'], dtype=torch.int64),\n",
    "                #'hydra_slim': torch.tensor(Esm2_embedding(''.join(['X' for n in range(sample['length'])])), dtype=torch.float32),\n",
    "                'hydra_slim': torch.tensor(sample['mut_type']*0, dtype=torch.float32),\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "# class DeltaDataset_nohydra(Dataset):\n",
    "#     def __init__(self, data, dim_embedding, inv = False):\n",
    "#         self.data = data\n",
    "#         self.dim_embedding = dim_embedding\n",
    "#         self.inv = inv\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sample = self.data[idx]\n",
    "\n",
    "#         if self.inv: \n",
    "#             return {\n",
    "#                 'id': sample['id'],\n",
    "#                 'wild_type': torch.tensor(sample['mut_type'], dtype=torch.float32),    # inverto mut con wild \n",
    "#                 'mut_type': torch.tensor(sample['wild_type'], dtype=torch.float32),    # inverto mut con wild             \n",
    "#                 'length': torch.tensor(sample['length'], dtype=torch.float32),\n",
    "#                 'ddg': torch.tensor(-float(sample['ddg']), dtype=torch.float32),       # -ddg\n",
    "#                 'pos_mut': torch.tensor(sample['pos_mut'], dtype=torch.int64),\n",
    "#                 'hydra_slim': torch.tensor(sample['wild_type'], dtype=torch.float32),\n",
    "#                 }\n",
    "\n",
    "#         else:\n",
    "#             return {\n",
    "#                 'id': sample['id'],\n",
    "#                 'wild_type': torch.tensor(sample['wild_type'], dtype=torch.float32),\n",
    "#                 'mut_type': torch.tensor(sample['mut_type'],dtype=torch.float32),\n",
    "#                 'length': torch.tensor(sample['length'], dtype=torch.float32),\n",
    "#                 'ddg': torch.tensor(float(sample['ddg']), dtype=torch.float32),\n",
    "#                 'pos_mut': torch.tensor(sample['pos_mut'], dtype=torch.int64),\n",
    "#                 'hydra_slim': torch.tensor(sample['wild_type'], dtype=torch.float32),\n",
    "#                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37194b0e-d9a3-4928-aa15-b510cbe7a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = max(sample['wild_type'].shape[0] for sample in batch)  # Max sequence length in batch   700\n",
    "    max_features = max(sample['wild_type'].shape[1] for sample in batch)  # Max feature size\n",
    "\n",
    "    padded_batch = {\n",
    "        'id': [],\n",
    "        'wild_type': [],\n",
    "        'mut_type': [],\n",
    "        'length': [],\n",
    "        'ddg': [],\n",
    "        'pos_mut': [],\n",
    "        'hydra_slim':[],\n",
    "    }\n",
    "\n",
    "    for sample in batch:\n",
    "        \n",
    "        wild_type_padded = F.pad(sample['wild_type'], (0, max_features - sample['wild_type'].shape[1], \n",
    "                                                       0, max_len - sample['wild_type'].shape[0]))\n",
    "        \n",
    "        mut_type_padded = F.pad(sample['mut_type'], (0, max_features - sample['mut_type'].shape[1], \n",
    "                                                     0, max_len - sample['mut_type'].shape[0]))\n",
    "        \n",
    "        hydra_slim_type_padded = F.pad(sample['hydra_slim'], (0, max_features - sample['hydra_slim'].shape[1], \n",
    "                                                       0, max_len - sample['hydra_slim'].shape[0]))        \n",
    "\n",
    "        padded_batch['id'].append(sample['id'])  \n",
    "        padded_batch['wild_type'].append(wild_type_padded)  \n",
    "        padded_batch['mut_type'].append(mut_type_padded)  \n",
    "        padded_batch['length'].append(sample['length'])\n",
    "        padded_batch['ddg'].append(sample['ddg'])\n",
    "        padded_batch['hydra_slim'].append(hydra_slim_type_padded)\n",
    "\n",
    "\n",
    "    # Convert list of tensors into a single batch tensor\n",
    "    padded_batch['wild_type'] = torch.stack(padded_batch['wild_type'])  # Shape: (batch_size, max_len, max_features)\n",
    "    padded_batch['mut_type'] = torch.stack(padded_batch['mut_type'])  \n",
    "    padded_batch['length'] = torch.stack(padded_batch['length'])  \n",
    "    padded_batch['ddg'] = torch.stack(padded_batch['ddg'])\n",
    "    padded_batch['hydra_slim'] = torch.stack(padded_batch['hydra_slim'])\n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "def dataloader_generation(path, collate_fn, batch_size = 128, dataloader_shuffle = True, inv= False):\n",
    "    \n",
    "    dim_embedding = 1280\n",
    "    dataset= []\n",
    "\n",
    "    for path in path:\n",
    "        with open(path, 'rb') as f:\n",
    "            dataset += pickle.load(f)\n",
    "\n",
    "    delta_dataset = DeltaDataset(dataset, dim_embedding, inv = inv)  \n",
    "    dataloader_delta = DataLoader(delta_dataset, batch_size=batch_size, shuffle=dataloader_shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader_delta\n",
    "\n",
    "# def dataloader_generation_nohydra(path, collate_fn, batch_size = 128, dataloader_shuffle = True, inv= False):\n",
    "    \n",
    "#     dim_embedding = 1280\n",
    "#     dataset= []\n",
    "\n",
    "#     for path in path:\n",
    "#         with open(path, 'rb') as f:\n",
    "#             dataset += pickle.load(f)\n",
    "\n",
    "#     delta_dataset = DeltaDataset_nohydra(dataset, dim_embedding, inv = inv)  \n",
    "#     dataloader_delta = DataLoader(delta_dataset, batch_size=batch_size, shuffle=dataloader_shuffle, collate_fn=collate_fn)\n",
    "\n",
    "#     return dataloader_delta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b306576-bd45-49d6-88f3-c90a2a6a97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader  # Use standard PyTorch DataLoader\n",
    "import random\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "train_path =[f's2450_fold_{i}_hydra_slim.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_hydra_slim_inv.pkl' for i in [0,1,2,3,4]]#+[f's2450_fold_{i}_hydra_slim.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_hydra_slim_inv.pkl' for i in [0,1,2,3,4]]\n",
    "\n",
    "\n",
    "#[f's2450_fold_{i}_hydra_slim_SHIFTED1.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_hydra_slim_SHIFTED1_inv.pkl' for i in [0,1,2,3,4]] #\n",
    "#[f's2450_fold_{i}.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_inv.pkl' for i in [0,1,2,3,4]]#['DA_th0.0_foldx_train.pkl'] + ['foldx_train.pkl'] + ['Double_mut_DA_0.0_foldx_train.pkl']+['test_TS16.pkl']#[f's2450_fold_{i}.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_inv.pkl' for i in [0,1,2,3,4]] + ['ptmul_train.pkl']+['DA_s2450.pkl'] #\n",
    "val_path = ['ptmul_train.pkl']#['s669_Castrense.pkl']#[f's2450_fold_{i}.pkl' for i in val_set]+[f's2450_fold_{i}_inv.pkl' for i in val_set]\n",
    "test_path = ['M28_test.pkl']#['s669_Castrense_hydra_slim.pkl']#['test_TS16.pkl']\n",
    "\n",
    "dataloader_train = dataloader_generation(path = train_path, batch_size = 6,collate_fn=collate_fn, dataloader_shuffle = True, inv= False)\n",
    "dataloader_validation = dataloader_generation(path = val_path, batch_size = 1, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "dataloader_test = dataloader_generation(path = test_path, batch_size = 1, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b46a1f-ded3-4ab0-bbcb-d91ab2483241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def output_model_from_batch(batch, model, device, hydra = False,train=False):\n",
    "    \n",
    "    x_wild = batch['wild_type'].float().to(device)\n",
    "    x_mut = batch['mut_type'].float().to(device)\n",
    "    hydra_slim = batch['hydra_slim'].float().to(device)\n",
    "    labels = batch['ddg'].float().to(device)\n",
    "    length = batch['length'].to(device)\n",
    "    output_ddg = model(x_wild, x_mut, hydra_slim, length, hydra=hydra, train = train)\n",
    "    \n",
    "    return output_ddg, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def training_and_validation_loop_ddg(model, dataloader_train, dataloader_test, dataloader_validation, path_save_fig, epochs=20, lr =0.001, patience=10):\n",
    "            \n",
    "    criterion =nn.MSELoss()# nn.HuberLoss()#nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    \n",
    "    pearson_r_train = []\n",
    "    pearson_r_test = []\n",
    "    pearson_r_validation = []\n",
    "    \n",
    "    loss_ddg_train = []\n",
    "    loss_ddg_train_TRANS = []\n",
    "    loss_ddg_train_TOT = []\n",
    "    \n",
    "    loss_ddg_validation = []\n",
    "    loss_ddg_validation_TRANS = []\n",
    "    loss_ddg_validation_TOT = []\n",
    "    \n",
    "    loss_ddg_test = []\n",
    "    loss_ddg_test_TRANS=[]\n",
    "    loss_ddg_test_TOT = []\n",
    "\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "            \n",
    "        # Training Loop\n",
    "        model.train()\n",
    "        preds_ddg_train = []\n",
    "        labels_tot_epoch = []\n",
    "\n",
    "        preds_ddg_train_TRANS = []\n",
    "        labels_tot_epoch_TRANS = []\n",
    "\n",
    "        for i, batch in enumerate(dataloader_train):\n",
    "            train = True\n",
    "            optimizer.zero_grad()\n",
    "            output_ddg_train, labels_train = output_model_from_batch(batch, model, device, hydra=False, train=True)\n",
    "            output_ddg_HYDRA_SLIM_train, _ = output_model_from_batch(batch, model, device, hydra=True, train=True)\n",
    "            \n",
    "            loss_ddg = criterion(output_ddg_train, labels_train)  #usa se NON uso hydra            \n",
    "            tot_loss = loss_ddg + criterion(output_ddg_HYDRA_SLIM_train, output_ddg_train)\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            tot_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Collect predictions\n",
    "            preds_ddg_train.extend(output_ddg_train.cpu().reshape(-1).tolist())\n",
    "            labels_tot_epoch.extend(labels_train.cpu().tolist())\n",
    "\n",
    "            preds_ddg_train_TRANS.extend(output_ddg_HYDRA_SLIM_train.cpu().reshape(-1).tolist())\n",
    "            labels_tot_epoch_TRANS.extend(output_ddg_train.cpu().tolist())            \n",
    "\n",
    "        # Calculate and print train metrics\n",
    "        train_loss = mean_squared_error(preds_ddg_train, labels_tot_epoch)\n",
    "        train_loss_TRANS = mean_squared_error(preds_ddg_train_TRANS, labels_tot_epoch_TRANS)\n",
    "        \n",
    "        train_correlation = pearsonr(preds_ddg_train, labels_tot_epoch)[0]\n",
    "        train_spearman = spearmanr(preds_ddg_train, labels_tot_epoch)[0]\n",
    "        \n",
    "        loss_ddg_train.append(train_loss)\n",
    "        loss_ddg_train_TRANS.append(train_loss_TRANS)\n",
    "        loss_ddg_train_TOT.append(train_loss_TRANS+train_loss)\n",
    "        pearson_r_train.append(train_correlation)\n",
    "        \n",
    "        # Validation Loop\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "                \n",
    "        all_preds_validation = []\n",
    "        all_labels_validation = []\n",
    "        all_preds_validation_TRANS = []\n",
    "\n",
    "        \n",
    "        all_preds_test = []\n",
    "        all_labels_test = []\n",
    "        all_preds_test_TRANS = []\n",
    "                \n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for i, batch in enumerate(dataloader_test):\n",
    "\n",
    "                output_ddg_test, labels_test = output_model_from_batch(batch, model, device, hydra=False, train=False) \n",
    "                output_ddg_HYDRA_SLIM_test, _ = output_model_from_batch(batch, model, device, hydra=True, train=False)      \n",
    "                    \n",
    "                all_preds_test.extend(output_ddg_test.cpu().reshape(-1).tolist())\n",
    "                all_labels_test.extend(labels_test.cpu().tolist())\n",
    "\n",
    "                all_preds_test_TRANS.extend(output_ddg_HYDRA_SLIM_test.cpu().reshape(-1).tolist())\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            test_loss = mean_squared_error(all_preds_test, all_labels_test)\n",
    "            loss_ddg_test.append(test_loss)\n",
    "\n",
    "            test_loss_TRANS = mean_squared_error(all_preds_test_TRANS, all_preds_test)\n",
    "            loss_ddg_test_TRANS.append(test_loss_TRANS)\n",
    "\n",
    "            loss_ddg_test_TOT.append(test_loss+test_loss_TRANS)\n",
    "            \n",
    "            test_correlation, _ = pearsonr(all_preds_test, all_labels_test)\n",
    "            pearson_r_test.append(test_correlation)\n",
    "\n",
    "            test_correlation_TRANS = pearsonr(all_preds_test_TRANS, all_preds_test)\n",
    "\n",
    "            for i, batch in enumerate(dataloader_validation):\n",
    "                output_ddg_validation, labels_validation = output_model_from_batch(batch, model, device, hydra=False, train=False,)#inizio = 'wild',fine='mut')\n",
    "                output_ddg_HYDRA_SLIM_validation, _ = output_model_from_batch(batch, model, device, hydra=True, train=False)      \n",
    "\n",
    "                all_preds_validation.extend(output_ddg_validation.cpu().reshape(-1).tolist())\n",
    "                all_labels_validation.extend(labels_validation.cpu().tolist()) #MESSO UN -  se DEF AL CONTRARIO\n",
    "\n",
    "                all_preds_validation_TRANS.extend(output_ddg_HYDRA_SLIM_validation.cpu().reshape(-1).tolist())\n",
    "\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            val_loss = mean_squared_error(all_preds_validation, all_labels_validation)\n",
    "            loss_ddg_validation.append(val_loss)\n",
    "\n",
    "            val_loss_TRANS = mean_squared_error(all_preds_validation_TRANS, all_preds_validation)\n",
    "            loss_ddg_validation_TRANS.append(val_loss_TRANS)\n",
    "\n",
    "            loss_ddg_validation_TOT.append(val_loss+val_loss_TRANS)\n",
    "            \n",
    "            \n",
    "            val_correlation, _ = pearsonr(all_preds_validation, all_labels_validation)\n",
    "            pearson_r_validation.append(val_correlation)\n",
    "\n",
    "        print(f'pearson tra triangolazione e non triangolazione : {test_correlation_TRANS}\\n')\n",
    "        print(f'pearson tra triangolazione e true ddg: {pearsonr(all_preds_test_TRANS, all_labels_test)}\\n')\n",
    "        \n",
    "        if val_correlation >= max(pearson_r_validation): \n",
    "            best_model = copy.deepcopy(model)\n",
    "            print(f'\\033[91mEpoch {epoch+1}/{num_epochs}')\n",
    "            print(f'Train -  trans_loss={train_loss_TRANS:.4f},    Loss: {train_loss:.4f}, Pearson r: {train_correlation:.4f}, Rho spearman: {train_spearman:.4f}')\n",
    "            print(f'Validation - Loss: {val_loss:.4f}, Pearson r: {val_correlation:.4f}, Rho spearman: {spearmanr(all_preds_validation, all_labels_validation)[0]:.4f}',)        \n",
    "            print(f'Test - trans_loss={test_loss_TRANS:.4f},      Loss: {test_loss:.4f}, Pearson r: {test_correlation:.4f}, Rho spearman: {spearmanr(all_preds_test, all_labels_test)[0]:.4f}\\033[0m\\n')\n",
    "      \n",
    "\n",
    "        else:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print(f'Train -    trans_loss={train_loss_TRANS:.4f},    Loss: {train_loss:.4f}, Pearson r: {train_correlation:.4f}, Rho spearman: {train_spearman:.4f}')\n",
    "            print(f'Validation - Loss: {val_loss:.4f}, Pearson r: {val_correlation:.4f}, Rho spearman: {spearmanr(all_preds_validation, all_labels_validation)[0]:.4f}',)        \n",
    "            print(f'Test -  trans_loss={test_loss_TRANS:.4f}      Loss: {test_loss:.4f}, Pearson r: {test_correlation:.4f}, Rho spearman: {spearmanr(all_preds_test, all_labels_test)[0]:.4f}\\n')\n",
    "                  \n",
    "        if epoch > (np.argmax(pearson_r_validation) + patience):\n",
    "            print(f'\\033[91mEarly stopping at epoch {epoch+1}\\033[0m')\n",
    "            break\n",
    "\n",
    "        # if (epoch == 100) or (epoch == 150) or (epoch == 200) or (epoch == 250) or (epoch == 300):\n",
    "        #     torch.save(model, f'JanusDDG_{epoch}_ensamble.pth')\n",
    "    \n",
    "    pearson_max_val = np.max(pearson_r_validation)\n",
    "\n",
    "    return pearson_r_train, pearson_r_validation, pearson_r_test, loss_ddg_train, loss_ddg_validation, loss_ddg_test, loss_ddg_train_TRANS, loss_ddg_validation_TRANS, loss_ddg_test_TRANS, loss_ddg_train_TOT, loss_ddg_validation_TOT, loss_ddg_test_TOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e35b0-73e5-455a-8047-ff4d81a531af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "929f7e6d-973d-4fb4-bf11-e89af78108e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Cross_Attention_DDG(nn.Module):\n",
    "    \n",
    "    def __init__(self, base_module, cross_att=False, dual_cross_att= False, hydra=True ,**transf_parameters):\n",
    "        super().__init__()\n",
    "        self.base_ddg = base_module(**transf_parameters, cross_att=cross_att, dual_cross_att= dual_cross_att).to(device)\n",
    "        self.hydra=hydra\n",
    "    \n",
    "    def forward(self, x_wild, x_mut, hydra_slim, length, hydra=False, train = True):\n",
    "\n",
    "        if train:\n",
    "            if hydra:\n",
    "                \n",
    "                # Calcolo DDG tra wild e primo intermezzo\n",
    "                delta_dir = x_wild - hydra_slim\n",
    "                wild_half_DDG = self.base_ddg(delta_dir, x_wild, length)\n",
    "                \n",
    "                # Calcolo DDG tra ultimo intermezzo e mutato\n",
    "                delta_dir = hydra_slim - x_mut\n",
    "                half_mut_DDG = self.base_ddg(delta_dir, hydra_slim, length)\n",
    "                \n",
    "                # Somma totale\n",
    "                output_TCA = wild_half_DDG + half_mut_DDG\n",
    "    \n",
    "            else:\n",
    "                # Calcolo DDG tra wild e primo intermezzo\n",
    "                delta_dir = x_wild - x_mut\n",
    "                output_TCA = self.base_ddg(delta_dir, x_wild, length)         \n",
    "    \n",
    "        else:\n",
    "            if hydra:\n",
    "                \n",
    "                # Calcolo DDG tra wild e primo intermezzo\n",
    "                delta_dir = x_wild - hydra_slim\n",
    "                delta_inv = hydra_slim - x_wild\n",
    "                wild_half_DDG = (self.base_ddg(delta_dir, x_wild, length) - self.base_ddg(delta_inv, hydra_slim, length)) / 2\n",
    "                \n",
    "                # Calcolo DDG tra ultimo intermezzo e mutato\n",
    "                delta_dir = hydra_slim - x_mut\n",
    "                delta_inv = x_mut - hydra_slim\n",
    "                half_mut_DDG = (self.base_ddg(delta_dir, hydra_slim, length) - self.base_ddg(delta_inv, x_mut, length)) / 2\n",
    "                \n",
    "                # Somma totale\n",
    "                output_TCA = wild_half_DDG + half_mut_DDG\n",
    "    \n",
    "            else:\n",
    "                # Calcolo DDG tra wild e primo intermezzo\n",
    "                delta_dir = x_wild - x_mut\n",
    "                delta_inv = x_mut - x_wild\n",
    "                #wild_half_DDG = (self.base_ddg(delta_dir, half_aas[0], length) - self.base_ddg(delta_inv, x_wild, length)) / 2\n",
    "                output_TCA = (self.base_ddg(delta_dir, x_wild, length) - self.base_ddg(delta_inv, x_mut, length)) / 2            \n",
    "            \n",
    "        return output_TCA  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c98febd2-26c0-4c75-9768-9a9871b263e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def apply_masked_pooling(position_attn_output, padding_mask):\n",
    "\n",
    "    # Convert mask to float for element-wise multiplication\n",
    "    padding_mask = padding_mask.float()\n",
    "\n",
    "    # Global Average Pooling (GAP) - Exclude padded tokens\n",
    "    # Sum only over valid positions (padding_mask is False for valid positions)\n",
    "    sum_output = torch.sum(position_attn_output * (1 - padding_mask.unsqueeze(-1)), dim=1)  # (batch_size, feature_dim)\n",
    "    valid_count = torch.sum((1 - padding_mask).float(), dim=1)  # (batch_size,)\n",
    "    gap = sum_output / valid_count.unsqueeze(-1)  # Divide by number of valid positions\n",
    "\n",
    "    # Global Max Pooling (GMP) - Exclude padded tokens\n",
    "    # Set padded positions to -inf so they don't affect the max computation\n",
    "    position_attn_output_masked = position_attn_output * (1 - padding_mask.unsqueeze(-1)) + (padding_mask.unsqueeze(-1) * (- 1e10))\n",
    "    gmp, _ = torch.max(position_attn_output_masked, dim=1)  # (batch_size, feature_dim)\n",
    "\n",
    "    return gap, gmp\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=3700):\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape (1, max_len, embedding_dim)\n",
    "        self.register_buffer('pe', pe)  # Salvato come tensore fisso (non parametro)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class TransformerRegression(nn.Module):\n",
    "    def __init__(self, input_dim=1280, num_heads=8, dropout_rate=0., num_experts=1, f_activation = nn.ReLU(), kernel_size=20, cross_att = True,\n",
    "                dual_cross_att=True):\n",
    "        \n",
    "        super(TransformerRegression, self).__init__()\n",
    "\n",
    "        self.embedding_dim = input_dim\n",
    "        self.act = f_activation\n",
    "        self.max_len = 3700 #lunghezza massima proteina\n",
    "        out_channels = 128  #num filtri conv 1D\n",
    "        kernel_size = 20\n",
    "        padding = 0\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=self.embedding_dim, \n",
    "                                             out_channels=out_channels, \n",
    "                                             kernel_size=kernel_size, \n",
    "                                             padding=padding) \n",
    "        \n",
    "        self.conv1d_wild = nn.Conv1d(in_channels=self.embedding_dim, \n",
    "                                             out_channels=out_channels, \n",
    "                                             kernel_size=kernel_size, \n",
    "                                             padding=padding)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(out_channels)\n",
    "        self.norm2 = nn.LayerNorm(out_channels)\n",
    "        \n",
    "        # Cross-attention layers\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(out_channels, 3700)\n",
    "        self.speach_att_type = True\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=out_channels, num_heads=num_heads, dropout=dropout_rate, batch_first=True )\n",
    "        self.inverse_attention = nn.MultiheadAttention(embed_dim=out_channels, num_heads=num_heads, dropout=dropout_rate, batch_first =True)\n",
    "        \n",
    "        dim_position_wise_FFN = out_channels*2\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(dim_position_wise_FFN)\n",
    "        self.router = nn.Linear(dim_position_wise_FFN, num_experts) #dim_position_wise_FFN*2\n",
    "\n",
    "        self.pw_ffnn = nn.Sequential(\n",
    "            nn.Linear(dim_position_wise_FFN, 512),\n",
    "            self.act,\n",
    "            nn.Linear(512, dim_position_wise_FFN)\n",
    "            )\n",
    "        \n",
    "\n",
    "        self.Linear_ddg = nn.Linear(dim_position_wise_FFN*2, 1)\n",
    "\n",
    "            \n",
    "\n",
    "    def create_padding_mask(self, length, seq_len, batch_size):\n",
    "        \"\"\"\n",
    "        Create a padding mask for multihead attention.\n",
    "        length: Tensor of shape (batch_size,) containing the actual lengths of the sequences.\n",
    "        seq_len: The maximum sequence length.\n",
    "        batch_size: The number of sequences in the batch.\n",
    "        \n",
    "        Returns a padding mask of shape (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        mask = torch.arange(seq_len, device=length.device).unsqueeze(0) >= length.unsqueeze(1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, delta_w_m, x_wild, length):\n",
    "            \n",
    "            delta_w_m = delta_w_m.transpose(1, 2)  # (batch_size, feature_dim, seq_len) -> (seq_len, batch_size, feature_dim)\n",
    "            C_delta_w_m = self.conv1d(delta_w_m)\n",
    "            C_delta_w_m = C_delta_w_m.transpose(1, 2)  # (seq_len, batch_size, feature_dim) -> (batch_size, seq_len, feature_dim)\n",
    "            C_delta_w_m = self.positional_encoding(C_delta_w_m)\n",
    "            \n",
    "            x_wild = x_wild.transpose(1, 2)  # (batch_size, feature_dim, seq_len) -> (seq_len, batch_size, feature_dim)\n",
    "            C_x_wild = self.conv1d_wild(x_wild)\n",
    "            C_x_wild = C_x_wild.transpose(1, 2)  # (seq_len, batch_size, feature_dim) -> (batch_size, seq_len, feature_dim)\n",
    "            C_x_wild = self.positional_encoding(C_x_wild)            \n",
    "            \n",
    "            batch_size, seq_len, feature_dim = C_x_wild.size()\n",
    "\n",
    "            padding_mask = self.create_padding_mask(length, seq_len, batch_size)        \n",
    "                    \n",
    "            if self.speach_att_type:\n",
    "                print('ATTENTION TYPE: Dual cross Attention\\n q = wild , k = delta, v = delta and q = delta , k = wild, v = wild \\n ----------------------------------')\n",
    "                self.speach_att_type = False\n",
    "                \n",
    "            direct_attn_output, _ = self.multihead_attention(C_x_wild, C_delta_w_m, C_delta_w_m, key_padding_mask=padding_mask)\n",
    "            direct_attn_output += C_delta_w_m \n",
    "            direct_attn_output = self.norm1(direct_attn_output)                        \n",
    "            \n",
    "            inverse_attn_output, _ = self.inverse_attention(C_delta_w_m, C_x_wild, C_x_wild, key_padding_mask=padding_mask)                   \n",
    "            inverse_attn_output += C_x_wild  \n",
    "            inverse_attn_output = self.norm2(inverse_attn_output)\n",
    "            \n",
    "            attn_output = torch.cat([direct_attn_output, inverse_attn_output], dim=-1)\n",
    "\n",
    "            output = self.pw_ffnn(attn_output)\n",
    "    \n",
    "            position_attn_output = attn_output + output\n",
    "    \n",
    "            position_attn_output = self.norm3(position_attn_output)\n",
    "    \n",
    "            gap, gmp = apply_masked_pooling(position_attn_output, padding_mask)\n",
    "    \n",
    "            # Concatenate GAP and GMP\n",
    "            pooled_output = torch.cat([gap, gmp], dim=-1)  # (batch_size, 2 * feature_dim)\n",
    "    \n",
    "            # Pass through FFNN to predict DDG\n",
    "            x = self.Linear_ddg(pooled_output)        \n",
    "            \n",
    "            return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d78e28f9-35f3-49c6-b2f6-6e53c433b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e868852-c219-4b8f-af00-87d01884432a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "245621e5-bf63-4d4f-a923-f1bac00c67ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_563775/2918905594.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Final_model = torch.load('JanusDDG_300epochs_ARXIVE.pth')#Cross_Attention_DDG(DDG_model, cross_att = True, dual_cross_att=True, **transf_parameters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDGemb \n",
      " ----------------------------------\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.8851722255810179, pvalue=3.974044070153077e-10)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.4639411055627248, pvalue=0.012888891490112912)\n",
      "\n",
      "\u001b[91mEpoch 1/28\n",
      "Train -  trans_loss=0.9823,    Loss: 0.6594, Pearson r: 0.9825, Rho spearman: 0.9929\n",
      "Validation - Loss: 4.2563, Pearson r: 0.5609, Rho spearman: 0.4918\n",
      "Test - trans_loss=0.3563,      Loss: 6.0229, Pearson r: 0.6649, Rho spearman: 0.6317\u001b[0m\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.8870922896436889, pvalue=3.2278810149658024e-10)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.44472164548481674, pvalue=0.017731226106191735)\n",
      "\n",
      "Epoch 2/28\n",
      "Train -    trans_loss=0.7961,    Loss: 0.7018, Pearson r: 0.9936, Rho spearman: 0.9924\n",
      "Validation - Loss: 4.1834, Pearson r: 0.5345, Rho spearman: 0.4619\n",
      "Test -  trans_loss=0.3859      Loss: 5.6940, Pearson r: 0.6606, Rho spearman: 0.6257\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.8578195433027164, pvalue=5.437951043494025e-09)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.4443714142368362, pvalue=0.01783172416558965)\n",
      "\n",
      "Epoch 3/28\n",
      "Train -    trans_loss=0.7532,    Loss: 0.6372, Pearson r: 0.9879, Rho spearman: 0.9862\n",
      "Validation - Loss: 4.1185, Pearson r: 0.5066, Rho spearman: 0.4338\n",
      "Test -  trans_loss=0.5086      Loss: 5.1193, Pearson r: 0.6497, Rho spearman: 0.5997\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.7758563501599659, pvalue=1.2299685552996348e-06)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.4200297016578681, pvalue=0.026059245735208143)\n",
      "\n",
      "Epoch 4/28\n",
      "Train -    trans_loss=0.6778,    Loss: 0.5132, Pearson r: 0.9785, Rho spearman: 0.9774\n",
      "Validation - Loss: 4.1871, Pearson r: 0.4879, Rho spearman: 0.4216\n",
      "Test -  trans_loss=0.6859      Loss: 4.6464, Pearson r: 0.6465, Rho spearman: 0.6065\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.8182096376072189, pvalue=1.0467378505408929e-07)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.4252299551221876, pvalue=0.024082797186156652)\n",
      "\n",
      "Epoch 5/28\n",
      "Train -    trans_loss=0.6038,    Loss: 0.4467, Pearson r: 0.9679, Rho spearman: 0.9714\n",
      "Validation - Loss: 4.2361, Pearson r: 0.4846, Rho spearman: 0.4188\n",
      "Test -  trans_loss=0.6082      Loss: 4.5843, Pearson r: 0.6381, Rho spearman: 0.5904\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.7834726156602885, pvalue=8.224108850469332e-07)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3977936980922672, pvalue=0.036050550242516695)\n",
      "\n",
      "Epoch 6/28\n",
      "Train -    trans_loss=0.5239,    Loss: 0.4146, Pearson r: 0.9616, Rho spearman: 0.9694\n",
      "Validation - Loss: 4.3100, Pearson r: 0.4835, Rho spearman: 0.4171\n",
      "Test -  trans_loss=0.8463      Loss: 4.3296, Pearson r: 0.6339, Rho spearman: 0.6002\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.7793777416307157, pvalue=1.0231034447886686e-06)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3893257833991709, pvalue=0.04058060477427263)\n",
      "\n",
      "Epoch 7/28\n",
      "Train -    trans_loss=0.5151,    Loss: 0.4058, Pearson r: 0.9592, Rho spearman: 0.9692\n",
      "Validation - Loss: 4.3057, Pearson r: 0.4829, Rho spearman: 0.4149\n",
      "Test -  trans_loss=0.7430      Loss: 4.3556, Pearson r: 0.6307, Rho spearman: 0.5898\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.7308314673161765, pvalue=1.0032752277047327e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.35838421067423465, pvalue=0.06111233651540336)\n",
      "\n",
      "Epoch 8/28\n",
      "Train -    trans_loss=0.4912,    Loss: 0.3883, Pearson r: 0.9587, Rho spearman: 0.9689\n",
      "Validation - Loss: 4.2890, Pearson r: 0.4875, Rho spearman: 0.4195\n",
      "Test -  trans_loss=0.9156      Loss: 4.3169, Pearson r: 0.6383, Rho spearman: 0.5931\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.698904805481589, pvalue=3.517048353227609e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3336295621546433, pvalue=0.08273730881048141)\n",
      "\n",
      "Epoch 9/28\n",
      "Train -    trans_loss=0.4685,    Loss: 0.3688, Pearson r: 0.9596, Rho spearman: 0.9705\n",
      "Validation - Loss: 4.3368, Pearson r: 0.4859, Rho spearman: 0.4179\n",
      "Test -  trans_loss=1.0062      Loss: 4.2448, Pearson r: 0.6375, Rho spearman: 0.6008\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.7015181458872736, pvalue=3.193171042751352e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3264499712622826, pvalue=0.08998763929672376)\n",
      "\n",
      "Epoch 10/28\n",
      "Train -    trans_loss=0.4545,    Loss: 0.3619, Pearson r: 0.9596, Rho spearman: 0.9712\n",
      "Validation - Loss: 4.3686, Pearson r: 0.4856, Rho spearman: 0.4170\n",
      "Test -  trans_loss=1.0423      Loss: 4.1698, Pearson r: 0.6421, Rho spearman: 0.6106\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.7227000477624869, pvalue=1.4032912049005706e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.36837982624394966, pvalue=0.053747955291405504)\n",
      "\n",
      "Epoch 11/28\n",
      "Train -    trans_loss=0.4388,    Loss: 0.3467, Pearson r: 0.9601, Rho spearman: 0.9720\n",
      "Validation - Loss: 4.4001, Pearson r: 0.4848, Rho spearman: 0.4162\n",
      "Test -  trans_loss=1.0635      Loss: 4.1431, Pearson r: 0.6405, Rho spearman: 0.6164\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6841355677497785, pvalue=5.9611887919565146e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3180947572681142, pvalue=0.09901945883920735)\n",
      "\n",
      "Epoch 12/28\n",
      "Train -    trans_loss=0.4214,    Loss: 0.3266, Pearson r: 0.9615, Rho spearman: 0.9723\n",
      "Validation - Loss: 4.4418, Pearson r: 0.4856, Rho spearman: 0.4164\n",
      "Test -  trans_loss=1.1734      Loss: 4.0632, Pearson r: 0.6460, Rho spearman: 0.6085\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6841573867053035, pvalue=5.956675713809897e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3376195116898755, pvalue=0.07890545521651224)\n",
      "\n",
      "Epoch 13/28\n",
      "Train -    trans_loss=0.4062,    Loss: 0.3161, Pearson r: 0.9625, Rho spearman: 0.9733\n",
      "Validation - Loss: 4.4646, Pearson r: 0.4889, Rho spearman: 0.4188\n",
      "Test -  trans_loss=1.2037      Loss: 4.0149, Pearson r: 0.6460, Rho spearman: 0.6085\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6890208328554097, pvalue=5.023339317777653e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.33406055440219945, pvalue=0.0823166895748593)\n",
      "\n",
      "Epoch 14/28\n",
      "Train -    trans_loss=0.3962,    Loss: 0.3073, Pearson r: 0.9628, Rho spearman: 0.9738\n",
      "Validation - Loss: 4.5488, Pearson r: 0.4869, Rho spearman: 0.4169\n",
      "Test -  trans_loss=1.2664      Loss: 3.9650, Pearson r: 0.6475, Rho spearman: 0.6041\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6257599351671876, pvalue=0.0003689588298976318)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.31792650262139516, pvalue=0.09920807424532015)\n",
      "\n",
      "Epoch 15/28\n",
      "Train -    trans_loss=0.3845,    Loss: 0.2935, Pearson r: 0.9640, Rho spearman: 0.9743\n",
      "Validation - Loss: 4.4001, Pearson r: 0.4952, Rho spearman: 0.4257\n",
      "Test -  trans_loss=1.3055      Loss: 3.9776, Pearson r: 0.6593, Rho spearman: 0.6156\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.66000344436996, pvalue=0.00013274724388629664)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.32353774855692063, pvalue=0.09306190987210268)\n",
      "\n",
      "Epoch 16/28\n",
      "Train -    trans_loss=0.3678,    Loss: 0.2812, Pearson r: 0.9654, Rho spearman: 0.9751\n",
      "Validation - Loss: 4.5332, Pearson r: 0.4910, Rho spearman: 0.4212\n",
      "Test -  trans_loss=1.4053      Loss: 3.9065, Pearson r: 0.6489, Rho spearman: 0.6063\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6597992301546645, pvalue=0.00013360939764587784)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3276675910026769, pvalue=0.0887253113416208)\n",
      "\n",
      "Epoch 17/28\n",
      "Train -    trans_loss=0.3581,    Loss: 0.2663, Pearson r: 0.9663, Rho spearman: 0.9756\n",
      "Validation - Loss: 4.5175, Pearson r: 0.4934, Rho spearman: 0.4219\n",
      "Test -  trans_loss=1.3107      Loss: 3.9580, Pearson r: 0.6518, Rho spearman: 0.6016\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6674993792794789, pvalue=0.00010431728485714507)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3565208525269272, pvalue=0.06256816582173057)\n",
      "\n",
      "Epoch 18/28\n",
      "Train -    trans_loss=0.3451,    Loss: 0.2556, Pearson r: 0.9677, Rho spearman: 0.9764\n",
      "Validation - Loss: 4.5769, Pearson r: 0.4920, Rho spearman: 0.4226\n",
      "Test -  trans_loss=1.4528      Loss: 3.8807, Pearson r: 0.6545, Rho spearman: 0.6049\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6005416082944797, pvalue=0.0007278524917455948)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.27814314348282576, pvalue=0.15181428006004563)\n",
      "\n",
      "Epoch 19/28\n",
      "Train -    trans_loss=0.3272,    Loss: 0.2423, Pearson r: 0.9690, Rho spearman: 0.9769\n",
      "Validation - Loss: 4.5563, Pearson r: 0.4960, Rho spearman: 0.4255\n",
      "Test -  trans_loss=1.6373      Loss: 3.8342, Pearson r: 0.6501, Rho spearman: 0.6101\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.7754545903283797, pvalue=1.2558217547366815e-06)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.4151241373116274, pvalue=0.02804264209749444)\n",
      "\n",
      "Epoch 20/28\n",
      "Train -    trans_loss=0.3253,    Loss: 0.2352, Pearson r: 0.9696, Rho spearman: 0.9778\n",
      "Validation - Loss: 4.4801, Pearson r: 0.5002, Rho spearman: 0.4302\n",
      "Test -  trans_loss=0.9527      Loss: 3.9033, Pearson r: 0.6622, Rho spearman: 0.5929\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.677701538254512, pvalue=7.432543034081785e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.35761410814669026, pvalue=0.06171077875202106)\n",
      "\n",
      "Epoch 21/28\n",
      "Train -    trans_loss=0.3093,    Loss: 0.2213, Pearson r: 0.9712, Rho spearman: 0.9783\n",
      "Validation - Loss: 4.5916, Pearson r: 0.4934, Rho spearman: 0.4243\n",
      "Test -  trans_loss=1.4330      Loss: 3.8446, Pearson r: 0.6531, Rho spearman: 0.6210\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6718826514876348, pvalue=9.032193534990618e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.333557400577106, pvalue=0.08280789346362122)\n",
      "\n",
      "Epoch 22/28\n",
      "Train -    trans_loss=0.3039,    Loss: 0.2159, Pearson r: 0.9719, Rho spearman: 0.9786\n",
      "Validation - Loss: 4.5970, Pearson r: 0.4959, Rho spearman: 0.4254\n",
      "Test -  trans_loss=1.4666      Loss: 3.7144, Pearson r: 0.6634, Rho spearman: 0.6246\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6291625428351707, pvalue=0.00033512436268655495)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.30601888094269186, pvalue=0.11325353896637197)\n",
      "\n",
      "Epoch 23/28\n",
      "Train -    trans_loss=0.2903,    Loss: 0.2078, Pearson r: 0.9726, Rho spearman: 0.9793\n",
      "Validation - Loss: 4.7146, Pearson r: 0.4919, Rho spearman: 0.4233\n",
      "Test -  trans_loss=1.6935      Loss: 3.7023, Pearson r: 0.6601, Rho spearman: 0.6356\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.6460400144907728, pvalue=0.00020444026586909187)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.32093595501229893, pvalue=0.09587495222276939)\n",
      "\n",
      "Epoch 24/28\n",
      "Train -    trans_loss=0.2795,    Loss: 0.1964, Pearson r: 0.9737, Rho spearman: 0.9803\n",
      "Validation - Loss: 4.5202, Pearson r: 0.5017, Rho spearman: 0.4321\n",
      "Test -  trans_loss=1.5202      Loss: 3.7428, Pearson r: 0.6639, Rho spearman: 0.6413\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.665468499089885, pvalue=0.00011142948910356041)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3185725320438033, pvalue=0.09848533547624634)\n",
      "\n",
      "Epoch 25/28\n",
      "Train -    trans_loss=0.2716,    Loss: 0.1908, Pearson r: 0.9744, Rho spearman: 0.9808\n",
      "Validation - Loss: 4.6224, Pearson r: 0.5001, Rho spearman: 0.4308\n",
      "Test -  trans_loss=1.5732      Loss: 3.7558, Pearson r: 0.6536, Rho spearman: 0.6287\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.7400064115045171, pvalue=6.771436156740735e-06)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.36663602355870506, pvalue=0.05497984604884314)\n",
      "\n",
      "Epoch 26/28\n",
      "Train -    trans_loss=0.2647,    Loss: 0.1852, Pearson r: 0.9751, Rho spearman: 0.9810\n",
      "Validation - Loss: 4.5772, Pearson r: 0.5016, Rho spearman: 0.4306\n",
      "Test -  trans_loss=1.2844      Loss: 3.7940, Pearson r: 0.6578, Rho spearman: 0.6328\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.678577094143607, pvalue=7.215023651839224e-05)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.32017432528774464, pvalue=0.09671041144265172)\n",
      "\n",
      "Epoch 27/28\n",
      "Train -    trans_loss=0.2650,    Loss: 0.1823, Pearson r: 0.9757, Rho spearman: 0.9813\n",
      "Validation - Loss: 4.4888, Pearson r: 0.5073, Rho spearman: 0.4375\n",
      "Test -  trans_loss=1.2531      Loss: 3.8854, Pearson r: 0.6602, Rho spearman: 0.6347\n",
      "\n",
      "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.7562308442418136, pvalue=3.2429552912578176e-06)\n",
      "\n",
      "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.3867127200989978, pvalue=0.04206657773787336)\n",
      "\n",
      "Epoch 28/28\n",
      "Train -    trans_loss=0.2605,    Loss: 0.1733, Pearson r: 0.9768, Rho spearman: 0.9818\n",
      "Validation - Loss: 4.4516, Pearson r: 0.5092, Rho spearman: 0.4388\n",
      "Test -  trans_loss=1.1414      Loss: 3.7903, Pearson r: 0.6733, Rho spearman: 0.6421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PROVA base base\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr = 1e-4\n",
    "#lr = 1e-5\n",
    "#lr = 1e-4\n",
    "\n",
    "input_dim = 1280\n",
    "\n",
    "transf_parameters={'input_dim':1280, 'num_heads':8,\n",
    "                    'dropout_rate':0.,}\n",
    "\n",
    "patience = 300\n",
    "DDG_model = TransformerRegression\n",
    "Final_model = torch.load('JanusDDG_300epochs_ARXIVE.pth')#Cross_Attention_DDG(DDG_model, cross_att = True, dual_cross_att=True, **transf_parameters)\n",
    "\n",
    "path_save_fig = 'DDGemb \\n ----------------------------------'\n",
    "print(path_save_fig)\n",
    "pearson_r_train, pearson_r_validation, pearson_r_test, loss_ddg_train, loss_ddg_validation, loss_ddg_test, loss_ddg_train_TRANS, loss_ddg_validation_TRANS, loss_ddg_test_TRANS, loss_ddg_train_TOT, loss_ddg_validation_TOT, loss_ddg_test_TOT = training_and_validation_loop_ddg(Final_model, dataloader_train, dataloader_test,\n",
    "                                                                                   dataloader_validation,\n",
    "                                                                                   path_save_fig, epochs=28, lr =lr,patience = patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12cb09bc-90e4-4e3a-acc9-988d0a6a59c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pearson_r_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa7abb-8a17-4691-8933-7d68609f10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.8851722255810179, pvalue=3.974044070153077e-10)\n",
    "\n",
    "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.4639411055627248, pvalue=0.012888891490112912)\n",
    "\n",
    "Epoch 1/28\n",
    "Train -  trans_loss=0.9823,    Loss: 0.6594, Pearson r: 0.9825, Rho spearman: 0.9929\n",
    "Validation - Loss: 4.2563, Pearson r: 0.5609, Rho spearman: 0.4918\n",
    "Test - trans_loss=0.3563,      Loss: 6.0229, Pearson r: 0.6649, Rho spearman: 0.6317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b856f-581c-4b41-8c9c-dea3f87b323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c9f2f-257d-4703-b85f-248571f8d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Creazione della figura con due sottografici affiancati\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\n",
    "\n",
    "# PRIMO GRAFICO: Pearson\n",
    "#sns.lineplot(pearson_r_train, label=\"Pearson s669\", ax=axes[0])\n",
    "sns.lineplot(pearson_r_test, label=\"Pearson s669\", ax=axes[0])\n",
    "sns.lineplot(pearson_r_validation, label=\"Pearson Ptmul-NR\", ax=axes[0])\n",
    "\n",
    "# Aggiunta delle linee orizzontali di riferimento\n",
    "axes[0].axhline(y=0.595, color='r', linestyle='--', label=\"y = 0.595\")\n",
    "axes[0].axhline(y=0.59, color='g', linestyle='--', label=\"y = 0.59\")\n",
    "axes[0].axhline(y=0.545, color='b', linestyle='--', label=\"y = 0.545\")\n",
    "axes[0].axhline(y=0.54, color='y', linestyle='--', label=\"y = 0.54\")\n",
    "\n",
    "axes[0].set_title(\"Pearson Correlation\")\n",
    "axes[0].legend()\n",
    "\n",
    "# SECONDO GRAFICO: Loss\n",
    "loss_ddg_train_BASE = loss_ddg_train[::2]   # Prende gli elementi con indice pari\n",
    "loss_ddg_train_TRANS = loss_ddg_train[1::2]  # Prende gli elementi con indice dispari\n",
    "\n",
    "sns.lineplot(loss_ddg_train_BASE, label=\"Loss train BASE\", ax=axes[1])\n",
    "sns.lineplot(loss_ddg_train_TRANS, label=\"Loss train TRANS\", ax=axes[1])\n",
    "sns.lineplot(loss_ddg_train_TOT, label=\"Loss train TOT\", ax=axes[1])\n",
    "\n",
    "sns.lineplot(loss_ddg_test, label=\"Loss s669 BASE\", ax=axes[1])\n",
    "sns.lineplot(loss_ddg_test_TRANS, label=\"Loss s669 TRANS\", ax=axes[1])\n",
    "sns.lineplot(loss_ddg_test_TOT, label=\"Loss s669 TOT\", ax=axes[1])\n",
    "\n",
    "sns.lineplot(loss_ddg_validation, label=\"Loss Ptmul-NR BASE\", ax=axes[1])\n",
    "sns.lineplot(loss_ddg_validation_TRANS, label=\"Loss Ptmul-NR TRANS\", ax=axes[1])\n",
    "sns.lineplot(loss_ddg_validation_TOT, label=\"Loss Ptmul-NR TOT\" , ax=axes[1])\n",
    "\n",
    "\n",
    "axes[1].set_title(\"Loss DDG\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Mostrare il grafico finale\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33181661-767c-4bf7-86c7-7d9d8ca78d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Creazione della figura con 2 righe e 3 colonne (Pearson sopra, Loss sotto)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10), sharex=True)\n",
    "\n",
    "### 1️⃣ GRAFICI PEARSON ###\n",
    "# TRAIN\n",
    "sns.lineplot(pearson_r_train, label=\"Pearson Train\", ax=axes[0, 0], color='blue')\n",
    "\n",
    "axes[0, 0].set_title(\"Train Pearson\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# TEST\n",
    "sns.lineplot(pearson_r_test, label=\"Pearson M28\", ax=axes[0, 1], color='blue')\n",
    "# axes[0, 1].axhline(y=0.595, color='r', linestyle='--')\n",
    "# axes[0, 1].axhline(y=0.59, color='g', linestyle='--')\n",
    "# axes[0, 1].axhline(y=0.545, color='b', linestyle='--')\n",
    "# axes[0, 1].axhline(y=0.54, color='y', linestyle='--')\n",
    "axes[0, 1].set_title(\"M28 Pearson\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# VALIDATION\n",
    "sns.lineplot(pearson_r_validation, label=\"Pearson PTMUL-TRAIN\", ax=axes[0, 2], color='blue')\n",
    "# axes[0, 2].axhline(y=0.595, color='r', linestyle='--')\n",
    "# axes[0, 2].axhline(y=0.59, color='g', linestyle='--')\n",
    "# axes[0, 2].axhline(y=0.545, color='b', linestyle='--')\n",
    "# axes[0, 2].axhline(y=0.54, color='y', linestyle='--')\n",
    "axes[0, 2].set_title(\"PTMUL-TRAIN Pearson\")\n",
    "axes[0, 2].legend()\n",
    "\n",
    "### 2️⃣ GRAFICI LOSS ###\n",
    "# TRAIN\n",
    "sns.lineplot(loss_ddg_train, label=\"Loss train BASE\", ax=axes[1, 0], color='blue')\n",
    "sns.lineplot(loss_ddg_train_TRANS, label=\"Loss train TRANS\", ax=axes[1, 0], color='orange')\n",
    "sns.lineplot(loss_ddg_train_TOT, label=\"Loss train TOT\", ax=axes[1, 0], color='green')\n",
    "axes[1, 0].set_title(\"Train Loss\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# TEST\n",
    "sns.lineplot(loss_ddg_test, label=\"Loss M28 BASE\", ax=axes[1, 1], color='blue')\n",
    "sns.lineplot(loss_ddg_test_TRANS, label=\"Loss M28 TRANS\", ax=axes[1, 1], color='orange')\n",
    "sns.lineplot(loss_ddg_test_TOT, label=\"Loss M28 TOT\", ax=axes[1, 1], color='green')\n",
    "axes[1, 1].set_title(\"M28 Loss\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# VALIDATION\n",
    "sns.lineplot(loss_ddg_validation, label=\"Loss PTMUL-TRAIN BASE\", ax=axes[1, 2], color='blue')\n",
    "sns.lineplot(loss_ddg_validation_TRANS, label=\"Loss PTMUL-TRAIN TRANS\", ax=axes[1, 2], color='orange')\n",
    "sns.lineplot(loss_ddg_validation_TOT, label=\"Loss PTMUL-TRAIN TOT\", ax=axes[1, 2], color='green')\n",
    "axes[1, 2].set_title(\"PTMUL-TRAIN Loss\")\n",
    "axes[1, 2].legend()\n",
    "\n",
    "# Migliorare il layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc454302-fedc-4faf-9a34-c26f7d9c70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(pearson_r_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774c30f-b233-45e4-a2db-41269489fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_r_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88640487-2c75-46ab-8ccb-afb2435174bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metrics_data_zeroes.pkl\", \"rb\") as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "print(loaded_data['pearson_r_validation'][27])\n",
    "print(loaded_data['pearson_r_test'][27])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec39c59c-6be5-4779-b208-ba2b2e48e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Definiamo tutte le liste da salvare\n",
    "data_to_save = {\n",
    "    \"pearson_r_train\": pearson_r_train,\n",
    "    \"pearson_r_test\": pearson_r_test,\n",
    "    \"pearson_r_validation\": pearson_r_validation,\n",
    "    \"loss_ddg_train\": loss_ddg_train,\n",
    "    \"loss_ddg_train_BASE\": loss_ddg_train,  # Indici pari\n",
    "    \"loss_ddg_train_TRANS\": loss_ddg_train_TRANS,  # Indici dispari\n",
    "    \"loss_ddg_train_TOT\": loss_ddg_train_TOT,\n",
    "    \"loss_ddg_test\": loss_ddg_test,\n",
    "    \"loss_ddg_test_TRANS\": loss_ddg_test_TRANS,\n",
    "    \"loss_ddg_test_TOT\": loss_ddg_test_TOT,\n",
    "    \"loss_ddg_validation\": loss_ddg_validation,\n",
    "    \"loss_ddg_validation_TRANS\": loss_ddg_validation_TRANS,\n",
    "    \"loss_ddg_validation_TOT\": loss_ddg_validation_TOT\n",
    "}\n",
    "\n",
    "# 🔹 SALVATAGGIO su file pickle\n",
    "with open(\"metrics_data_zeroes.pkl\", \"wb\") as file:\n",
    "    pickle.dump(data_to_save, file)\n",
    "\n",
    "print(\"✅ Dati salvati in 'metrics_data.pkl' con successo!\")\n",
    "\n",
    "# 🔹 RICARICAMENTO per verifica\n",
    "with open(\"metrics_data_zeroes.pkl\", \"rb\") as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "print(\"✅ Dati ricaricati con successo!\")\n",
    "print(\"Chiavi disponibili:\", loaded_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c116f8c-754c-4bb1-bea2-45b764b67d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ddg_train_TOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6539d8-3da3-4be5-88b1-5aa4a5fb8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(Final_model, 'JanusDDG_28epochs_finetuned_zeros_MODELLO_FINALE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e952136-63d8-44ab-acad-b5b7e3682d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Final_model, 'JanusDDG_fine_tuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5677c-2416-43df-acd8-87447e976075",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ciao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66923e-0f94-4735-b6ad-de0ced338f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# lr = 1e-5\n",
    "# train_path =[f's2450_fold_{i}_hydra_slim_SHIFTED1.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_hydra_slim_SHIFTED1_inv.pkl' for i in [0,1,2,3,4]]\n",
    "# val_path = ['s669_Castrense_hydra_slim.pkl']#['s669_Castrense.pkl']#[f's2450_fold_{i}.pkl' for i in val_set]+[f's2450_fold_{i}_inv.pkl' for i in val_set]\n",
    "# test_path = ['s669_Castrense_hydra_slim.pkl']#['s669_Castrense_hydra_slim.pkl']#['test_TS16.pkl']\n",
    "# dataloader_train = dataloader_generation(path = train_path, batch_size = 6,collate_fn=collate_fn, dataloader_shuffle = True, inv= False)\n",
    "# dataloader_validation = dataloader_generation(path = val_path, batch_size = 1, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "# dataloader_test = dataloader_generation(path = test_path, batch_size = 1, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "            # output_ddg_train, labels_train = output_model_from_batch(batch, model, device, hydra=False, train=False)\n",
    "            # output_ddg_HYDRA_SLIM_train, _ = output_model_from_batch(batch, model, device, hydra=True, train=False)\n",
    "\n",
    "# DDGemb \n",
    "#  ----------------------------------\n",
    "# pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9588071372114625, pvalue=0.0)\n",
    "\n",
    "# pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.529450390983784, pvalue=1.3203795258932152e-49)\n",
    "\n",
    "# Epoch 1/100\n",
    "# Train -  trans_loss=0.2259,    Loss: 0.0191, Pearson r: 0.9992, Rho spearman: 0.9992\n",
    "# Validation - Loss: 1.9125, Pearson r: 0.5449, Rho spearman: 0.5671\n",
    "# Test - trans_loss=0.0645,      Loss: 1.9125, Pearson r: 0.5449, Rho spearman: 0.5671\n",
    "\n",
    "# pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.96427994120678, pvalue=0.0)\n",
    "\n",
    "# pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5319033664525803, pvalue=3.9247209250389655e-50)\n",
    "\n",
    "# Epoch 2/100\n",
    "# Train -  trans_loss=0.1719,    Loss: 0.0237, Pearson r: 0.9985, Rho spearman: 0.9985\n",
    "# Validation - Loss: 1.9049, Pearson r: 0.5456, Rho spearman: 0.5671\n",
    "# Test - trans_loss=0.0568,      Loss: 1.9049, Pearson r: 0.5456, Rho spearman: 0.5671\n",
    "\n",
    "# pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9675307802090347, pvalue=0.0)\n",
    "\n",
    "# pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5334596332503565, pvalue=1.8083434971915834e-50)\n",
    "\n",
    "# Epoch 3/100\n",
    "# Train -  trans_loss=0.1426,    Loss: 0.0232, Pearson r: 0.9978, Rho spearman: 0.9981\n",
    "# Validation - Loss: 1.9022, Pearson r: 0.5457, Rho spearman: 0.5672\n",
    "# Test - trans_loss=0.0521,      Loss: 1.9022, Pearson r: 0.5457, Rho spearman: 0.5672\n",
    "\n",
    "\n",
    "\n",
    "##################\n",
    "##################\n",
    "# \n",
    "# lr = 1e-4\n",
    "# train_path =[f's2450_fold_{i}_hydra_slim_SHIFTED1.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_hydra_slim_SHIFTED1_inv.pkl' for i in [0,1,2,3,4]]\n",
    "# val_path = ['s669_Castrense_hydra_slim.pkl']#['s669_Castrense.pkl']#[f's2450_fold_{i}.pkl' for i in val_set]+[f's2450_fold_{i}_inv.pkl' for i in val_set]\n",
    "# test_path = ['s669_Castrense_hydra_slim.pkl']#['s669_Castrense_hydra_slim.pkl']#['test_TS16.pkl']\n",
    "# dataloader_train = dataloader_generation(path = train_path, batch_size = 6,collate_fn=collate_fn, dataloader_shuffle = True, inv= False)\n",
    "# dataloader_validation = dataloader_generation(path = val_path, batch_size = 1, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "# dataloader_test = dataloader_generation(path = test_path, batch_size = 1, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "            # output_ddg_train, labels_train = output_model_from_batch(batch, model, device, hydra=False, train=False)\n",
    "            # output_ddg_HYDRA_SLIM_train, _ = output_model_from_batch(batch, model, device, hydra=True, train=False)\n",
    "\n",
    "\n",
    "# DDGemb \n",
    "#  ----------------------------------\n",
    "# pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9771304453887012, pvalue=0.0)\n",
    "\n",
    "# pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5287687980041857, pvalue=1.846449274271924e-49)\n",
    "\n",
    "# Epoch 1/100\n",
    "# Train -  trans_loss=0.1348,    Loss: 0.0303, Pearson r: 0.9965, Rho spearman: 0.9971\n",
    "# Validation - Loss: 1.9438, Pearson r: 0.5371, Rho spearman: 0.5625\n",
    "# Test - trans_loss=0.0333,      Loss: 1.9438, Pearson r: 0.5371, Rho spearman: 0.5625\n",
    "\n",
    "# pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9779563155914957, pvalue=0.0)\n",
    "\n",
    "# pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5248583635712959, pvalue=1.246010758494287e-48)\n",
    "\n",
    "# Epoch 2/100\n",
    "# Train -    trans_loss=0.0476,    Loss: 0.0235, Pearson r: 0.9965, Rho spearman: 0.9970\n",
    "# Validation - Loss: 1.9273, Pearson r: 0.5343, Rho spearman: 0.5602\n",
    "# Test -  trans_loss=0.0364      Loss: 1.9273, Pearson r: 0.5343, Rho spearman: 0.5602\n",
    "\n",
    "\n",
    "#########################\n",
    "#########################\n",
    "# SOTTO TRAIN CON  TRAIN = TRUE\n",
    "# lr = 1e-5\n",
    "# train_path =[f's2450_fold_{i}_hydra_slim_SHIFTED1.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_hydra_slim_SHIFTED1_inv.pkl' for i in [0,1,2,3,4]]\n",
    "# val_path = ['s669_Castrense_hydra_slim.pkl']#['s669_Castrense.pkl']#[f's2450_fold_{i}.pkl' for i in val_set]+[f's2450_fold_{i}_inv.pkl' for i in val_set]\n",
    "# test_path = ['s669_Castrense_hydra_slim.pkl']#['s669_Castrense_hydra_slim.pkl']#['test_TS16.pkl']\n",
    "# dataloader_train = dataloader_generation(path = train_path, batch_size = 6,collate_fn=collate_fn, dataloader_shuffle = True, inv= False)\n",
    "# dataloader_validation = dataloader_generation(path = val_path, batch_size = 1, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "# dataloader_test = dataloader_generation(path = test_path, batch_size = 1, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "            # output_ddg_train, labels_train = output_model_from_batch(batch, model, device, hydra=False, train=True)\n",
    "            # output_ddg_HYDRA_SLIM_train, _ = output_model_from_batch(batch, model, device, hydra=True, train=True)\n",
    "\n",
    "\n",
    "DDGemb \n",
    " ----------------------------------\n",
    "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9566641535294127, pvalue=0.0)\n",
    "\n",
    "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5277396095591367, pvalue=3.0592915998106224e-49)\n",
    "\n",
    "Epoch 1/100\n",
    "Train -  trans_loss=0.2762,    Loss: 0.0218, Pearson r: 0.9988, Rho spearman: 0.9989\n",
    "Validation - Loss: 1.9221, Pearson r: 0.5441, Rho spearman: 0.5663\n",
    "Test - trans_loss=0.0660,      Loss: 1.9221, Pearson r: 0.5441, Rho spearman: 0.5663\n",
    "\n",
    "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9610431624147351, pvalue=0.0)\n",
    "\n",
    "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5294964590627442, pvalue=1.290753378238322e-49)\n",
    "\n",
    "Epoch 2/100\n",
    "Train -  trans_loss=0.2115,    Loss: 0.0338, Pearson r: 0.9985, Rho spearman: 0.9982\n",
    "Validation - Loss: 1.9127, Pearson r: 0.5446, Rho spearman: 0.5663\n",
    "Test - trans_loss=0.0605,      Loss: 1.9127, Pearson r: 0.5446, Rho spearman: 0.5663\n",
    "\n",
    "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9640050340567555, pvalue=0.0)\n",
    "\n",
    "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5307356800858392, pvalue=7.001034432792789e-50)\n",
    "\n",
    "Epoch 3/100\n",
    "Train -  trans_loss=0.1839,    Loss: 0.0300, Pearson r: 0.9981, Rho spearman: 0.9978\n",
    "Validation - Loss: 1.9063, Pearson r: 0.5450, Rho spearman: 0.5669\n",
    "Test - trans_loss=0.0568,      Loss: 1.9063, Pearson r: 0.5450, Rho spearman: 0.5669\n",
    "\n",
    "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9661287212983981, pvalue=0.0)\n",
    "\n",
    "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5318791816748505, pvalue=3.9721413760502284e-50)\n",
    "\n",
    "Epoch 4/100\n",
    "Train -  trans_loss=0.1613,    Loss: 0.0279, Pearson r: 0.9977, Rho spearman: 0.9975\n",
    "Validation - Loss: 1.9020, Pearson r: 0.5453, Rho spearman: 0.5667\n",
    "Test - trans_loss=0.0544,      Loss: 1.9020, Pearson r: 0.5453, Rho spearman: 0.5667\n",
    "\n",
    "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9674782908934236, pvalue=0.0)\n",
    "\n",
    "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5322556619700721, pvalue=3.2944397234713975e-50)\n",
    "\n",
    "Epoch 5/100\n",
    "Train -  trans_loss=0.1427,    Loss: 0.0258, Pearson r: 0.9976, Rho spearman: 0.9975\n",
    "Validation - Loss: 1.8991, Pearson r: 0.5454, Rho spearman: 0.5670\n",
    "Test - trans_loss=0.0529,      Loss: 1.8991, Pearson r: 0.5454, Rho spearman: 0.5670\n",
    "\n",
    "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9686013014612453, pvalue=0.0)\n",
    "\n",
    "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.532149442873165, pvalue=3.4730707910935084e-50)\n",
    "\n",
    "Epoch 6/100\n",
    "Train -    trans_loss=0.1268,    Loss: 0.0241, Pearson r: 0.9974, Rho spearman: 0.9974\n",
    "Validation - Loss: 1.9022, Pearson r: 0.5451, Rho spearman: 0.5667\n",
    "Test -  trans_loss=0.0504      Loss: 1.9022, Pearson r: 0.5451, Rho spearman: 0.5667\n",
    "\n",
    "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9692794166233685, pvalue=0.0)\n",
    "\n",
    "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5322163147865776, pvalue=3.3595208299555624e-50)\n",
    "\n",
    "Epoch 7/100\n",
    "Train -    trans_loss=0.1115,    Loss: 0.0236, Pearson r: 0.9973, Rho spearman: 0.9975\n",
    "Validation - Loss: 1.8996, Pearson r: 0.5452, Rho spearman: 0.5664\n",
    "Test -  trans_loss=0.0500      Loss: 1.8996, Pearson r: 0.5452, Rho spearman: 0.5664\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228947cb-0d2a-4e4c-b9bd-707a15446ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66357b36-01d6-42f7-86c1-97f9183f9b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd570afd-dcef-42af-8806-718e9e6e1ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa11157-f356-4301-9863-a93f5db5575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DDGemb \n",
    " ----------------------------------\n",
    "pearson tra triangolazione e non triangolazione : PearsonRResult(statistic=0.9455839781553668, pvalue=0.0)\n",
    "\n",
    "pearson tra triangolazione e true ddg: PearsonRResult(statistic=0.5151058755154342, pvalue=1.3092470725534226e-46)\n",
    "\n",
    "Epoch 1/100\n",
    "Train -  trans_loss=0.9060,    Loss: 0.7454, Pearson r: 0.9846, Rho spearman: 0.9960\n",
    "Validation - Loss: 2.3962, Pearson r: 0.5331, Rho spearman: 0.5574\n",
    "Test - trans_loss=0.0252,      Loss: 2.3962, Pearson r: 0.5331, Rho spearman: 0.5574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452cc65-85f2-4225-88de-80b863cc95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance_test(model, dataloader_test, hydra=False):\n",
    "    # Assicurati che il modello sia in modalità di valutazione\n",
    "    model.eval()\n",
    "    \n",
    "    # Lista per salvare tutte le predizioni\n",
    "    all_predictions_test = []\n",
    "    all_lables_test = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "       \n",
    "        for i, batch in enumerate(dataloader_test):\n",
    "\n",
    "            predictions_test, labels_test = output_model_from_batch(batch, model, device, hydra=hydra)\n",
    "\n",
    "            # Aggiungi le predizioni alla lista\n",
    "            all_predictions_test.append(predictions_test)\n",
    "            all_lables_test.append(labels_test)\n",
    "    \n",
    "    return all_predictions_test, all_lables_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fd27c-5ca0-426e-9914-e16e9561544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_model = torch.load('JanusDDG_300epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4d2ce-2d9b-436a-8b53-ac3c5a65d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###SECONDA PROVA\n",
    "\n",
    "\n",
    "# class DeltaDataset(Dataset):\n",
    "#     def __init__(self, data, dim_embedding, inv = False):\n",
    "#         self.data = data\n",
    "#         self.dim_embedding = dim_embedding\n",
    "#         self.inv = inv\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sample = self.data[idx]\n",
    "\n",
    "#         if self.inv: \n",
    "#             return {\n",
    "#                 'id': sample['id'],\n",
    "#                 #'wild_type': torch.tensor(sample['mut_type'], dtype=torch.float32),    #inverto mut con wild \n",
    "#                 'wild_type': torch.tensor(sample['mut_type'], dtype=torch.float32),    #inverto mut con wild\n",
    "#                 #'wild_type': torch.tensor(sample['hydra_slim'], dtype=torch.float32),    #inverto mut con wild \n",
    "\n",
    "#                 #'mut_type': torch.tensor(sample['wild_type'], dtype=torch.float32),    #inverto mut con wild\n",
    "#                 'mut_type': torch.tensor(sample['hydra_slim'], dtype=torch.float32),    #inverto mut con wild             \n",
    "\n",
    "#                 'length': torch.tensor(sample['length'], dtype=torch.float32),\n",
    "#                 'ddg': torch.tensor(-float(sample['ddg']), dtype=torch.float32),       # -ddg\n",
    "#                 'pos_mut': torch.tensor(sample['pos_mut'], dtype=torch.int64),\n",
    "#                 'hydra_slim': torch.tensor(sample['hydra_slim'], dtype=torch.float32),\n",
    "#                 }\n",
    "\n",
    "#         else:\n",
    "#             return {\n",
    "#                 'id': sample['id'],\n",
    "#                 #'wild_type': torch.tensor(sample['wild_type'], dtype=torch.float32),\n",
    "#                 'wild_type': torch.tensor(sample['hydra_slim'], dtype=torch.float32),\n",
    "#                 'mut_type': torch.tensor(sample['mut_type'],dtype=torch.float32),\n",
    "#                 #'mut_type': torch.tensor(sample['hydra_slim'],dtype=torch.float32),\n",
    "\n",
    "#                 #'mut_type': torch.tensor(sample['mut_type'],dtype=torch.float32),\n",
    "#                 'length': torch.tensor(sample['length'], dtype=torch.float32),\n",
    "#                 'ddg': torch.tensor(float(sample['ddg']), dtype=torch.float32),\n",
    "#                 'pos_mut': torch.tensor(sample['pos_mut'], dtype=torch.int64),\n",
    "#                 'hydra_slim': torch.tensor(sample['hydra_slim'], dtype=torch.float32),\n",
    "#                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d8a04-ce3e-45d7-a6cf-200bb342a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = ['s669_Castrense_hydra_slim.pkl']\n",
    "\n",
    "dataloader_test_dir = dataloader_generation(path = path_test, batch_size = 6, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "all_predictions_test_dir, all_lables_test_dir = model_performance_test(Final_model,dataloader_test_dir,hydra=False)                        \n",
    "all_predictions_test_dir = pd.Series(torch.cat(all_predictions_test_dir, dim=0).cpu())\n",
    "all_lables_test_dir = pd.Series(torch.cat(all_lables_test_dir, dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e96cb-db63-4ff8-8e6b-2e2ca6442a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test_inv = dataloader_generation(path = path_test, batch_size = 6, collate_fn=collate_fn, dataloader_shuffle = False, inv= True)\n",
    "all_predictions_test_inv, all_lables_test_inv =model_performance_test(Final_model,dataloader_test_inv,hydra=False)                                \n",
    "all_predictions_test_inv = pd.Series(torch.cat(all_predictions_test_inv, dim=0).cpu())\n",
    "all_lables_test_inv = pd.Series(torch.cat(all_lables_test_inv, dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114609b2-2420-456b-ab18-63203e012892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0897d76-c3c3-47cd-82aa-e5cd5f82b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(all_predictions_test_dir,all_lables_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d72f4-9699-4f88-8e60-a4b887745065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDIZIONI_DIRETTE = all_predictions_test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ccb027-04af-4ac1-9073-763ea01180eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDIZIONI_1_PASSO = (all_predictions_test_dir-all_predictions_test_inv)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52c5e3-d0d4-4a38-ab5c-24d2acabdc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDIZIONI_1_PASSO = PREDIZIONI_DIRETTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa5f27-99c4-4c89-be1c-23fa8e8fcff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDIZIONI_2_PASSO = all_predictions_test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9eb65-24f2-4e2f-8d04-4b1702fdb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(PREDIZIONI_1_PASSO+PREDIZIONI_2_PASSO, PREDIZIONI_DIRETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44200eb7-5f6a-4480-ab25-445ad732d9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60cb82-5264-4298-8a0d-d196b220dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = ['s669_Castrense_hydra_slim.pkl']\n",
    "\n",
    "\n",
    "dataloader_test_dir = dataloader_generation(path = path_test, batch_size = 1, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "all_predictions_test_dir, all_lables_test_dir = model_performance_test(Final_model,dataloader_test_dir,hydra=True)                        \n",
    "all_predictions_test_dir = pd.Series(torch.cat(all_predictions_test_dir, dim=0).cpu())\n",
    "all_lables_test_dir = pd.Series(torch.cat(all_lables_test_dir, dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79165f47-5787-4064-8bfb-efc61c0b3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(all_predictions_test_dir, pd.read_csv('../git_JANUS_DDG/Results/Result_s669_to_process.csv')['DDG_JanusDDG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd800568-cc15-4f4e-99c9-7e325289a6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261e992-655d-455e-8a32-ff6f7dabc830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d1476-a855-47fc-9de0-e87f804d1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir =True#True\n",
    "inv = True\n",
    "path_test = ['s669_Castrense_hydra_slim.pkl']#['s669_hydra_Castrense.pkl']#['Ssym_correct_by_KORPM.pkl']#['s461_Castrense.pkl']#['s669_Castrense.pkl']#['../DeltaDelta_BELLO/cdna117k_fold_1.pkl'] + ['../DeltaDelta_BELLO/cdna117k_fold_2.pkl']#['s669_Castrense.pkl']\n",
    "#['dataset_doppie.pkl']\n",
    "all_predictions_test_dir=None\n",
    "all_lables_test_dir=None\n",
    "all_predictions_test_inv=None\n",
    "all_lables_test_inv=None\n",
    "\n",
    "dir_predictions=[]\n",
    "dir_lables=[]\n",
    "\n",
    "inv_predictions=[]\n",
    "inv_lables=[]\n",
    "\n",
    "\n",
    "if dir:\n",
    "    dataloader_test_dir = dataloader_generation(path = path_test, batch_size = 6, collate_fn=collate_fn, dataloader_shuffle = False, inv= False)\n",
    "    all_predictions_test_dir, all_lables_test_dir = model_performance_test(Final_model,dataloader_test_dir,hydra=False)                        \n",
    "    all_predictions_test_dir = pd.Series(torch.cat(all_predictions_test_dir, dim=0).cpu())\n",
    "    all_lables_test_dir = pd.Series(torch.cat(all_lables_test_dir, dim=0).cpu())\n",
    "    dir_predictions.append(all_predictions_test_dir)\n",
    "    dir_lables.append(all_lables_test_dir)\n",
    "\n",
    "if inv:\n",
    "    dataloader_test_inv = dataloader_generation(path = path_test, batch_size = 6, collate_fn=collate_fn, dataloader_shuffle = False, inv= True)\n",
    "    all_predictions_test_inv, all_lables_test_inv =model_performance_test(Final_model,dataloader_test_inv,hydra=False)                                \n",
    "    all_predictions_test_inv = pd.Series(torch.cat(all_predictions_test_inv, dim=0).cpu())\n",
    "    all_lables_test_inv = pd.Series(torch.cat(all_lables_test_inv, dim=0).cpu())\n",
    "    inv_predictions.append(all_predictions_test_inv)\n",
    "    inv_lables.append(all_lables_test_inv)\n",
    "    if ~dir:\n",
    "        all_lables_test_dir = -all_lables_test_inv\n",
    "\n",
    "#metrics(all_predictions_test_dir,all_predictions_test_inv, all_lables_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14973149-0354-4a52-9e45-cdd131f82f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_predictions_test_dir_HYDRA, all_lables_test_dir_HYDRA = model_performance_test(Final_model,dataloader_test_dir,hydra=True)                        \n",
    "all_predictions_test_dir_HYDRA = pd.Series(torch.cat(all_predictions_test_dir_HYDRA, dim=0).cpu())\n",
    "all_lables_test_dir_HYDRA = pd.Series(torch.cat(all_lables_test_dir_HYDRA, dim=0).cpu())\n",
    "\n",
    "\n",
    "\n",
    "all_predictions_test_inv_HYDRA, all_lables_test_inv_HYDRA =model_performance_test(Final_model,dataloader_test_inv,hydra=True)                                \n",
    "all_predictions_test_inv_HYDRA = pd.Series(torch.cat(all_predictions_test_inv_HYDRA, dim=0).cpu())\n",
    "all_lables_test_inv_HYDRA = pd.Series(torch.cat(all_lables_test_inv_HYDRA, dim=0).cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9a36f-af56-4724-a7e7-a45e697dcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr((all_predictions_test_dir-all_predictions_test_inv)/2, (all_predictions_test_dir_HYDRA-all_predictions_test_inv_HYDRA)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dc63d-6e3d-4833-a577-a0b6af83384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr((all_predictions_test_dir-all_predictions_test_inv)/2, (all_predictions_test_dir_HYDRA-all_predictions_test_inv_HYDRA)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1398fa-bf4d-4965-a6c9-0f61efb0ecd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5a585-9bc3-45a9-a191-d5dd1102f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYDRA_1_PASSO = (all_predictions_test_dir-all_predictions_test_inv)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51cf59-f481-4c30-8bf9-9e407694c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYDRA_2_PASSO =  (all_predictions_test_dir-all_predictions_test_inv)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd101f60-dedb-40f1-8124-b1ccd1ac2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(HYDRA_1_PASSO, all_lables_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c369133-41f6-4d71-809f-a209359aa6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(HYDRA_1_PASSO+HYDRA_2_PASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafad297-ed39-49cf-9eae-1eb75f5cfbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x =all_lables_test_dir,y=PREDIZIONE_DIRETTA )\n",
    "sns.scatterplot(x =all_lables_test_dir,y=HYDRA_1_PASSO+HYDRA_2_PASSO )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147eb026-5f9e-42bb-b932-c5f68a421776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDIZIONE_DIRETTA = (all_predictions_test_dir-all_predictions_test_inv)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5fa38-ebeb-491f-928f-bc789af53c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772e3bf-3c66-4d15-aa10-b16153802f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([p_TRANS,l_TRANS,lab_true]).to_csv('p_trans_l_trans_TRAIN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e5f8b-2216-44c3-be65-14adc9e02f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([p_TRANS,l_TRANS,lab_true]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae0586-9931-4dfc-8ac8-acc03f4f47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(pd.read_csv('../git_JANUS_DDG/Results/Result_s669_to_process.csv')['DDG_JanusDDG'], pd.read_csv('p_trans_l_trans_TRAIN.csv')['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece08e87-ec89-4668-98cc-db305b0a5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_TRANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2d0ba-74c0-4ce7-8303-9a35c7ddfbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_TRANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27853e1-d81b-4c6f-abc3-a6aee895ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([p_TRANS,l_TRANS,lab_true]).T.rename(columns={0:'pred_trans',1:'pred',2:'true'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871214ac-4d5e-4620-84d2-4efed338dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame([p_TRANS,l_TRANS,lab_true]).T.rename(columns={0:'pred_trans',1:'pred',2:'true'})\n",
    "d.to_csv('p_trans_l_trans_TRAIN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0c024-6cf0-4246-8b55-f87676974309",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr([-0.14371299743652344, -0.3913414478302002, -0.4835125207901001, 0.04273629188537598, -0.025092124938964844, 0.10471123456954956, -0.21423470973968506, -0.004149198532104492, -0.11719012260437012, -1.5348817110061646],\n",
    "        [-1.326751708984375, -1.0506248474121094, -1.758748173713684, 0.08321564644575119, -0.9314181804656982, 0.11959119886159897, -0.8220889568328857, -0.9604212045669556, -0.46827802062034607, -1.5829718112945557])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f2fc2-28a4-43d3-afa4-82bafd2a1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Final_model, 'JanusDDG_3epochs_finetuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676bf84e-c2b8-4273-92a9-5e067413087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOLD0\n",
    "Epoch 1/50\n",
    "Train -  trans_loss=1.0051,    Loss: 0.6700, Pearson r: 0.9803, Rho spearman: 0.9930\n",
    "Validation - Loss: 2.3989, Pearson r: 0.5375, Rho spearman: 0.5636\n",
    "Test - trans_loss=0.8241,      Loss: 0.7336, Pearson r: 0.9964, Rho spearman: 0.9964\n",
    "\n",
    "Epoch 2/50\n",
    "Train -  trans_loss=0.8126,    Loss: 0.7286, Pearson r: 0.9963, Rho spearman: 0.9963\n",
    "Validation - Loss: 2.2764, Pearson r: 0.5382, Rho spearman: 0.5629\n",
    "Test - trans_loss=0.8360,      Loss: 0.7141, Pearson r: 0.9960, Rho spearman: 0.9952\n",
    "\n",
    "Epoch 3/50\n",
    "Train -  trans_loss=0.7881,    Loss: 0.6885, Pearson r: 0.9932, Rho spearman: 0.9926\n",
    "Validation - Loss: 2.2742, Pearson r: 0.5474, Rho spearman: 0.5675\n",
    "Test - trans_loss=0.9066,      Loss: 0.6478, Pearson r: 0.9933, Rho spearman: 0.9910\n",
    "\n",
    "Epoch 4/50\n",
    "Train -  trans_loss=0.7462,    Loss: 0.6107, Pearson r: 0.9879, Rho spearman: 0.9875\n",
    "Validation - Loss: 2.1597, Pearson r: 0.5489, Rho spearman: 0.5638\n",
    "Test - trans_loss=0.9318,      Loss: 0.6003, Pearson r: 0.9849, Rho spearman: 0.9793\n",
    "\n",
    "Epoch 5/50\n",
    "Train -  trans_loss=0.6601,    Loss: 0.5096, Pearson r: 0.9769, Rho spearman: 0.9780\n",
    "Validation - Loss: 2.0195, Pearson r: 0.5551, Rho spearman: 0.5708\n",
    "Test - trans_loss=1.1015,      Loss: 0.4820, Pearson r: 0.9784, Rho spearman: 0.9711\n",
    "\n",
    "Epoch 6/50\n",
    "Train -    trans_loss=0.5753,    Loss: 0.4252, Pearson r: 0.9677, Rho spearman: 0.9718\n",
    "Validation - Loss: 2.0257, Pearson r: 0.5517, Rho spearman: 0.5637\n",
    "Test -  trans_loss=0.9866      Loss: 0.5066, Pearson r: 0.9758, Rho spearman: 0.9682\n",
    "\n",
    "Epoch 7/50\n",
    "Train -    trans_loss=0.5163,    Loss: 0.3995, Pearson r: 0.9613, Rho spearman: 0.9690\n",
    "Validation - Loss: 2.1413, Pearson r: 0.5520, Rho spearman: 0.5645\n",
    "Test -  trans_loss=1.0329      Loss: 0.5064, Pearson r: 0.9751, Rho spearman: 0.9684\n",
    "\n",
    "Epoch 8/50\n",
    "Train -    trans_loss=0.4697,    Loss: 0.3746, Pearson r: 0.9607, Rho spearman: 0.9703\n",
    "Validation - Loss: 1.9638, Pearson r: 0.5467, Rho spearman: 0.5570\n",
    "Test -  trans_loss=1.1225      Loss: 0.4557, Pearson r: 0.9745, Rho spearman: 0.9686\n",
    "\n",
    "Epoch 9/50\n",
    "Train -    trans_loss=0.4549,    Loss: 0.3532, Pearson r: 0.9612, Rho spearman: 0.9708\n",
    "Validation - Loss: 2.0069, Pearson r: 0.5480, Rho spearman: 0.5602\n",
    "Test -  trans_loss=1.0320      Loss: 0.4329, Pearson r: 0.9736, Rho spearman: 0.9673\n",
    "\n",
    "Epoch 10/50\n",
    "Train -    trans_loss=0.4402,    Loss: 0.3430, Pearson r: 0.9612, Rho spearman: 0.9710\n",
    "Validation - Loss: 2.1210, Pearson r: 0.5482, Rho spearman: 0.5616\n",
    "Test -  trans_loss=1.0447      Loss: 0.4585, Pearson r: 0.9732, Rho spearman: 0.9675\n",
    "\n",
    "Epoch 11/50\n",
    "Train -    trans_loss=0.4231,    Loss: 0.3225, Pearson r: 0.9624, Rho spearman: 0.9720\n",
    "Validation - Loss: 2.1139, Pearson r: 0.5457, Rho spearman: 0.5569\n",
    "Test -  trans_loss=1.0647      Loss: 0.4612, Pearson r: 0.9724, Rho spearman: 0.9667\n",
    "\n",
    "Epoch 12/50\n",
    "Train -    trans_loss=0.4001,    Loss: 0.3117, Pearson r: 0.9632, Rho spearman: 0.9728\n",
    "Validation - Loss: 1.9700, Pearson r: 0.5481, Rho spearman: 0.5611\n",
    "Test -  trans_loss=1.2254      Loss: 0.3882, Pearson r: 0.9715, Rho spearman: 0.9652\n",
    "\n",
    "Epoch 13/50\n",
    "Train -    trans_loss=0.3953,    Loss: 0.3048, Pearson r: 0.9635, Rho spearman: 0.9734\n",
    "Validation - Loss: 2.0333, Pearson r: 0.5436, Rho spearman: 0.5555\n",
    "Test -  trans_loss=1.2169      Loss: 0.3918, Pearson r: 0.9728, Rho spearman: 0.9670\n",
    "\n",
    "Epoch 14/50\n",
    "Train -    trans_loss=0.3848,    Loss: 0.2875, Pearson r: 0.9655, Rho spearman: 0.9752\n",
    "Validation - Loss: 1.9745, Pearson r: 0.5484, Rho spearman: 0.5619\n",
    "Test -  trans_loss=1.2349      Loss: 0.3598, Pearson r: 0.9740, Rho spearman: 0.9682\n",
    "\n",
    "Epoch 15/50\n",
    "Train -    trans_loss=0.3699,    Loss: 0.2698, Pearson r: 0.9672, Rho spearman: 0.9756\n",
    "Validation - Loss: 1.9451, Pearson r: 0.5485, Rho spearman: 0.5589\n",
    "Test -  trans_loss=1.2406      Loss: 0.3384, Pearson r: 0.9714, Rho spearman: 0.9648\n",
    "\n",
    "Epoch 16/50\n",
    "Train -    trans_loss=0.3592,    Loss: 0.2555, Pearson r: 0.9687, Rho spearman: 0.9765\n",
    "Validation - Loss: 2.0307, Pearson r: 0.5489, Rho spearman: 0.5580\n",
    "Test -  trans_loss=1.3852      Loss: 0.3271, Pearson r: 0.9720, Rho spearman: 0.9656\n",
    "\n",
    "Epoch 17/50\n",
    "Train -    trans_loss=0.3400,    Loss: 0.2458, Pearson r: 0.9692, Rho spearman: 0.9765\n",
    "Validation - Loss: 2.0573, Pearson r: 0.5476, Rho spearman: 0.5574\n",
    "Test -  trans_loss=1.3532      Loss: 0.3319, Pearson r: 0.9698, Rho spearman: 0.9613\n",
    "\n",
    "\n",
    "#FOLD1\n",
    "Epoch 1/50\n",
    "Train -  trans_loss=1.0213,    Loss: 0.6567, Pearson r: 0.9786, Rho spearman: 0.9919\n",
    "Validation - Loss: 2.3328, Pearson r: 0.5326, Rho spearman: 0.5588\n",
    "Test - trans_loss=0.8726,      Loss: 0.7057, Pearson r: 0.9975, Rho spearman: 0.9979\n",
    "\n",
    "Epoch 2/50\n",
    "Train -  trans_loss=0.7989,    Loss: 0.7327, Pearson r: 0.9959, Rho spearman: 0.9961\n",
    "Validation - Loss: 2.2474, Pearson r: 0.5411, Rho spearman: 0.5646\n",
    "Test - trans_loss=0.9015,      Loss: 0.6788, Pearson r: 0.9976, Rho spearman: 0.9976\n",
    "\n",
    "Epoch 3/50\n",
    "Train -  trans_loss=0.7955,    Loss: 0.6960, Pearson r: 0.9947, Rho spearman: 0.9947\n",
    "Validation - Loss: 2.2197, Pearson r: 0.5426, Rho spearman: 0.5622\n",
    "Test - trans_loss=0.8991,      Loss: 0.6783, Pearson r: 0.9964, Rho spearman: 0.9955\n",
    "\n",
    "Epoch 4/50\n",
    "Train -  trans_loss=0.7607,    Loss: 0.6404, Pearson r: 0.9902, Rho spearman: 0.9900\n",
    "Validation - Loss: 2.1576, Pearson r: 0.5451, Rho spearman: 0.5616\n",
    "Test - trans_loss=0.9809,      Loss: 0.6245, Pearson r: 0.9937, Rho spearman: 0.9911\n",
    "\n",
    "#FOLD2\n",
    "Epoch 1/50\n",
    "Train -  trans_loss=0.9933,    Loss: 0.6544, Pearson r: 0.9779, Rho spearman: 0.9905\n",
    "Validation - Loss: 2.4072, Pearson r: 0.5332, Rho spearman: 0.5592\n",
    "Test - trans_loss=0.8146,      Loss: 0.8195, Pearson r: 0.9968, Rho spearman: 0.9973\n",
    "\n",
    "Epoch 2/50\n",
    "Train -  trans_loss=0.7967,    Loss: 0.7431, Pearson r: 0.9960, Rho spearman: 0.9957\n",
    "Validation - Loss: 2.3575, Pearson r: 0.5432, Rho spearman: 0.5669\n",
    "Test - trans_loss=0.8477,      Loss: 0.7497, Pearson r: 0.9979, Rho spearman: 0.9983\n",
    "\n",
    "Epoch 3/50\n",
    "Train -  trans_loss=0.7892,    Loss: 0.7186, Pearson r: 0.9955, Rho spearman: 0.9948\n",
    "Validation - Loss: 2.2764, Pearson r: 0.5464, Rho spearman: 0.5667\n",
    "Test - trans_loss=0.8429,      Loss: 0.7175, Pearson r: 0.9962, Rho spearman: 0.9961\n",
    "\n",
    "Epoch 4/50\n",
    "Train -  trans_loss=0.7676,    Loss: 0.6752, Pearson r: 0.9937, Rho spearman: 0.9927\n",
    "Validation - Loss: 2.2042, Pearson r: 0.5521, Rho spearman: 0.5690\n",
    "Test - trans_loss=0.9290,      Loss: 0.6196, Pearson r: 0.9911, Rho spearman: 0.9895\n",
    "\n",
    "\n",
    "#SOTTO HO USATO FOLD 4 NON DI 6... PROVA CON 6\n",
    "#FOLD VAL 3\n",
    "Epoch 1/50\n",
    "Train -  trans_loss=0.9583,    Loss: 0.6801, Pearson r: 0.9810, Rho spearman: 0.9916\n",
    "Validation - Loss: 2.4221, Pearson r: 0.5380, Rho spearman: 0.5647\n",
    "Test - trans_loss=0.8525,      Loss: 0.7802, Pearson r: 0.9974, Rho spearman: 0.9980\n",
    "\n",
    "Epoch 2/50\n",
    "Train -  trans_loss=0.7983,    Loss: 0.7384, Pearson r: 0.9948, Rho spearman: 0.9933\n",
    "Validation - Loss: 2.3831, Pearson r: 0.5480, Rho spearman: 0.5710\n",
    "Test - trans_loss=0.8355,      Loss: 0.7686, Pearson r: 0.9967, Rho spearman: 0.9971\n",
    "\n",
    "Epoch 3/50\n",
    "Train -  trans_loss=0.7731,    Loss: 0.6979, Pearson r: 0.9925, Rho spearman: 0.9906\n",
    "Validation - Loss: 2.2818, Pearson r: 0.5509, Rho spearman: 0.5685\n",
    "Test - trans_loss=0.9439,      Loss: 0.6446, Pearson r: 0.9928, Rho spearman: 0.9933\n",
    "\n",
    "Epoch 4/50\n",
    "Train -  trans_loss=0.7341,    Loss: 0.6146, Pearson r: 0.9844, Rho spearman: 0.9809\n",
    "Validation - Loss: 2.2227, Pearson r: 0.5523, Rho spearman: 0.5673\n",
    "Test - trans_loss=0.9851,      Loss: 0.5983, Pearson r: 0.9786, Rho spearman: 0.9809\n",
    "\n",
    "Epoch 5/50\n",
    "Train -    trans_loss=0.6154,    Loss: 0.4760, Pearson r: 0.9712, Rho spearman: 0.9683\n",
    "Validation - Loss: 1.9757, Pearson r: 0.5462, Rho spearman: 0.5624\n",
    "Test -  trans_loss=1.2552      Loss: 0.4895, Pearson r: 0.9611, Rho spearman: 0.9621\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fold val 4 \n",
    "Epoch 1/50\n",
    "Train -  trans_loss=0.9633,    Loss: 0.7264, Pearson r: 0.9832, Rho spearman: 0.9923\n",
    "Validation - Loss: 2.4758, Pearson r: 0.5391, Rho spearman: 0.5653\n",
    "Test - trans_loss=0.5034,      Loss: 0.5450, Pearson r: 0.9960, Rho spearman: 0.9975\n",
    "\n",
    "Epoch 2/50\n",
    "Train -  trans_loss=0.8204,    Loss: 0.7638, Pearson r: 0.9944, Rho spearman: 0.9937\n",
    "Validation - Loss: 2.2981, Pearson r: 0.5420, Rho spearman: 0.5653\n",
    "Test - trans_loss=0.4875,      Loss: 0.5152, Pearson r: 0.9947, Rho spearman: 0.9957\n",
    "\n",
    "Epoch 3/50\n",
    "Train -  trans_loss=0.8100,    Loss: 0.7233, Pearson r: 0.9916, Rho spearman: 0.9893\n",
    "Validation - Loss: 2.3350, Pearson r: 0.5457, Rho spearman: 0.5637\n",
    "Test - trans_loss=0.4898,      Loss: 0.4965, Pearson r: 0.9885, Rho spearman: 0.9901\n",
    "\n",
    "Epoch 4/50\n",
    "Train -  trans_loss=0.7602,    Loss: 0.6453, Pearson r: 0.9842, Rho spearman: 0.9825\n",
    "Validation - Loss: 2.1598, Pearson r: 0.5485, Rho spearman: 0.5621\n",
    "Test - trans_loss=0.6440,      Loss: 0.4266, Pearson r: 0.9716, Rho spearman: 0.9780\n",
    "\n",
    "Epoch 5/50\n",
    "Train -    trans_loss=0.6597,    Loss: 0.5232, Pearson r: 0.9709, Rho spearman: 0.9721\n",
    "Validation - Loss: 2.0976, Pearson r: 0.5470, Rho spearman: 0.5613\n",
    "Test -  trans_loss=0.6680      Loss: 0.3780, Pearson r: 0.9572, Rho spearman: 0.9708\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66fb38-93f2-4836-81d5-bc191b8b0808",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot([\n",
    "    0.9685, 0.8048, 0.7809, 0.7176, 0.6070,\n",
    "    0.5258, 0.4876, 0.4547, 0.4229, 0.4095,\n",
    "    0.4016, 0.3825, 0.3685, 0.3515, 0.3472,\n",
    "    0.3227, 0.3129, 0.3053, 0.2971, 0.2757,\n",
    "    0.2846, 0.2682, 0.2608, 0.2552, 0.2385\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46deecd2-d303-4290-ab70-12b8de5333b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#s2450 +s2450inv+ ptmul_train : \n",
    "\n",
    "# Epoch 111/300\n",
    "# Train -      Loss: 0.0096, Pearson r: 0.9987, Rho spearman: 0.9984\n",
    "# Validation - Loss: 4.6144, Pearson r: 0.5621, Rho spearman: 0.5345\n",
    "# Test -       Loss: 1.9337, Pearson r: 0.5208, Rho spearman: 0.5338\n",
    "\n",
    "# Epoch 112/300\n",
    "# Train -      Loss: 0.0107, Pearson r: 0.9985, Rho spearman: 0.9983\n",
    "# Validation - Loss: 4.6020, Pearson r: 0.5630, Rho spearman: 0.5403\n",
    "# Test -       Loss: 1.9419, Pearson r: 0.5208, Rho spearman: 0.5367\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "train con hydra\n",
    "# Epoch 145/300\n",
    "# Train -      Loss: 0.0034, Pearson r: 0.9995, Rho spearman: 0.9995\n",
    "# Validation - Loss: 0.9176, Pearson r: 0.8205, Rho spearman: 0.8206\n",
    "# Test -       Loss: 2.0290, Pearson r: 0.5171, Rho spearman: 0.5325\n",
    "\n",
    "# Epoch 146/300\n",
    "# Train -      Loss: 0.0040, Pearson r: 0.9994, Rho spearman: 0.9994\n",
    "# Validation - Loss: 0.9274, Pearson r: 0.8196, Rho spearman: 0.8189\n",
    "# Test -       Loss: 2.0288, Pearson r: 0.5173, Rho spearman: 0.5318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872aaa6-d7ed-4792-99a0-faf83b6c2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Final_model, 'JanusDDG_300epochs_plus25_hydra_slim.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac2fb1-c821-489e-850f-e51b6659dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(Final_model, 'JanusDDG_300_all_train.pth') #window 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64401710-5d96-4a70-9a7f-de99a1a36395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(best_model, 'DDGemb_Cross_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02337b38-b682-44fd-a284-d0a3344460a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3542a-d235-4b44-ad6f-5ca161d01859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Lista dei file dei modelli salvati\n",
    "model_paths = [f'JanusDDG_{epoch}_ensamble.pth' for epoch in range(100, 301,50)]\n",
    "\n",
    "# Carica gli state_dict dei modelli\n",
    "state_dicts = [torch.load(path).state_dict() for path in model_paths]\n",
    "\n",
    "# Crea un nuovo state_dict per il modello mediato\n",
    "avg_state_dict = {}\n",
    "\n",
    "# Itera su tutti i parametri del modello\n",
    "for key in state_dicts[0]:  # Prendi le chiavi dal primo modello\n",
    "    avg_state_dict[key] = sum(d[key] for d in state_dicts) / len(state_dicts)\n",
    "\n",
    "# Carica i pesi mediati in un nuovo modello\n",
    "final_model = torch.load(model_paths[0])  # Carica uno dei modelli per l'architettura\n",
    "final_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "# Salva il modello mediato\n",
    "torch.save(final_model, \"JanusDDG_avg_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6713aa-f17c-4810-8192-6d1f7dd46fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "con \n",
    "\n",
    "#SENZA RELU NELLA CONV1D\n",
    "# lr = 1e-4\n",
    "# input_dim = 1280\n",
    "\n",
    "# transf_parameters={'input_dim':1280, 'num_heads':8,\n",
    "#                     'dropout_rate':0.,}\n",
    "# arrivo a 0.54  (dopo un po' meno di 200 epoche)\n",
    "\n",
    "\n",
    "#     def __init__(self, input_dim=1280, num_heads=8, dropout_rate=0., num_experts=1, f_activation = nn.ReLU(), kernel_size=15, cross_att = False,\n",
    "#                 dual_cross_att=False):\n",
    "        \n",
    "#         super(TransformerRegression, self).__init__()\n",
    "#         self.cross_att = cross_att\n",
    "#         self.dual_cross_att = dual_cross_att\n",
    "        \n",
    "#         print(f'Cross Attention: {cross_att}')\n",
    "#         print(f'Dual Cross Attention: {dual_cross_att}')\n",
    "\n",
    "#         self.embedding_dim = input_dim\n",
    "#         self.act = f_activation\n",
    "#         self.max_len = 3700 #lunghezza massima proteina\n",
    "#         out_channels = 128  #num filtri conv 1D\n",
    "#         kernel_size = 20\n",
    "#         padding = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae41c28-e7e6-4c53-ab9c-bb12e35e4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"JanusDDG_loss_train.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(l_tr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342216a-bd92-4b60-ba85-de5a6a321fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=False)\n",
    "\n",
    "# Primo sottografico: Pearson r per il set di Train\n",
    "sns.lineplot(data=p_tr, ax=axes[0], label='Train r')\n",
    "#sns.lineplot(data=p_val, ax=axes[0], label='Test r')\n",
    "sns.lineplot(data=p_te, ax=axes[0], label='Test r')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Pearson r Values for Train and Test Set')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Pearson r')\n",
    "#axes[0].text(10, 0.53, str(round(pearson_max_val,3)), fontsize=12, color='red')\n",
    "axes[0].text(10, 0.53, str(round(max(p_te),3)), fontsize=12, color='red')\n",
    "\n",
    "axes[0].axhline(y=0.545, color='r', linestyle='--', linewidth=2)\n",
    "\n",
    "# Secondo sottografico: Pearson r per il set di Test\n",
    "sns.lineplot(data=l_tr, ax=axes[1], label='Train Loss')\n",
    "sns.lineplot(data=l_te, ax=axes[1], label='Test Loss')\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Loss Values for Train and Test Set')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Huber Loss')\n",
    "\n",
    "# Imposta il titolo generale per la figura\n",
    "fig.suptitle('JanusDDG Pearson r Values for Train and Test Sets', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73b1d9-b5b4-497e-baa4-af130826e9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74861c-0515-4b8f-82bb-51ab1d153b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08a31a-a4a5-4768-873b-3b59bdaf3acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf6c16-ab15-4ef7-9fbf-a446c602f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b2738-25e5-4dd5-b9dd-1119798ef5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred_dir, pred_inv, true):\n",
    "\n",
    "    #Dirette\n",
    "    print(f'Pearson test dirette: {pearsonr(true,pred_dir)[0]}')   \n",
    "    print(f'Spearmanr test dirette: {spearmanr(true,pred_dir)[0]}')    \n",
    "    print(f'RMSE dirette: {root_mean_squared_error(true,pred_dir)}')\n",
    "    print(f'MAE dirette: {mean_absolute_error(true,pred_dir)}\\n')\n",
    "    \n",
    "    #Inverse\n",
    "    print(f'Pearson test inverse: {pearsonr(-true,pred_inv)[0]}')   \n",
    "    print(f'Spearmanr test inverse: {spearmanr(-true,pred_inv)[0]}')    \n",
    "    print(f'RMSE inverse: {root_mean_squared_error(-true,pred_inv)}')\n",
    "    print(f'MAE inverse: {mean_absolute_error(-true,pred_inv)}\\n')\n",
    "    #Tot\n",
    "    \n",
    "    print(f'Pearson test tot: {pearsonr(pd.concat([true,-true],axis=0),pd.concat([pred_dir,pred_inv],axis=0))[0]}')   \n",
    "    print(f'Spearmanr test tot: {spearmanr(pd.concat([true,-true],axis=0),pd.concat([pred_dir,pred_inv],axis=0))[0]}')    \n",
    "    print(f'RMSE tot: {root_mean_squared_error(pd.concat([true,-true],axis=0),pd.concat([pred_dir,pred_inv],axis=0))}')\n",
    "    print(f'MAE tot: {mean_absolute_error(pd.concat([true,-true],axis=0),pd.concat([pred_dir,pred_inv],axis=0))}\\n')\n",
    "    \n",
    "    print(f'PCC d-r: {pearsonr(pred_dir,pred_inv)}\\n')\n",
    "    print(f'anti-symmetry bias: {np.mean(pred_dir + pred_inv)}\\n-----------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bdcdc-5cbf-44fd-ac0f-5de90b55c836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42ae80-6c55-4d6e-a362-24d5af1bb279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output_model_from_batch_inv(batch, model, device, train=True):\n",
    "\n",
    "#     '''Dato un modello pytorch e batch restituisce: output_modello, True labels'''\n",
    "    \n",
    "#     x_wild = batch['mut_type'].float().to(device)\n",
    "#     x_mut = batch['wild_type'].float().to(device)\n",
    "#     labels = -batch['ddg'].float().to(device)\n",
    "#     length = batch['length'].to(device)\n",
    "#     output_ddg = model(x_wild, x_mut, length, train = train)\n",
    "    \n",
    "#     return output_ddg, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb6056-ecb1-4295-a449-4be5219c5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_generation_pred(E_TYPE, test_path, batch_size = 128, dataloader_shuffle = True, inv= False):\n",
    "    \n",
    "    EMBEDDING_TYPE = E_TYPE\n",
    "    \n",
    "    if EMBEDDING_TYPE == 'ESM2':\n",
    "\n",
    "        '''train formato da s2648 + UnionV e DA; 1000 dei DA sono usati nel validation insieme a s669 DA\n",
    "        '''\n",
    "        \n",
    "        dim_embedding = 1280\n",
    "        \n",
    "        dataset_test = []\n",
    "        \n",
    "        for path in test_path:           \n",
    "            with open(path, 'rb') as f:\n",
    "                dataset_test += pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    dataset_test = DeltaDataset(dataset_test, dim_embedding, inv = inv)\n",
    "    \n",
    "    # Creazione DataLoader\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=dataloader_shuffle, collate_fn=collate_fn)#collate_fn_MULTIPLE\n",
    "\n",
    "    return dataloader_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fa9c3-c6ac-49bb-ade2-f999a1d8fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def collate_fn_MULTIPLE(batch):\n",
    "#     max_len = max(sample['wild_type'].shape[0] for sample in batch)  # Max sequence length in batch   700\n",
    "#     max_features = max(sample['wild_type'].shape[1] for sample in batch)  # Max feature size\n",
    "\n",
    "#     padded_batch = {\n",
    "#         'id': [],\n",
    "#         'wild_type': [],\n",
    "#         'mut_type': [],\n",
    "#         'length': [],\n",
    "#         'ddg': [],\n",
    "#         #'alpha_vec': [],\n",
    "#         'pos_mut': []\n",
    "#     }\n",
    "\n",
    "#     for sample in batch:\n",
    "#         wild_type_padded = F.pad(sample['wild_type'], (0, max_features - sample['wild_type'].shape[1], \n",
    "#                                                        0, max_len - sample['wild_type'].shape[0]))\n",
    "#         mut_type_padded = F.pad(sample['mut_type'], (0, max_features - sample['mut_type'].shape[1], \n",
    "#                                                      0, max_len - sample['mut_type'].shape[0]))\n",
    "\n",
    "#         padded_batch['id'].append(sample['id'])  \n",
    "#         padded_batch['wild_type'].append(wild_type_padded)  \n",
    "#         padded_batch['mut_type'].append(mut_type_padded)  \n",
    "#         padded_batch['length'].append(sample['length'])#append(torch.tensor(sample['length'], dtype=torch.float32))  \n",
    "#         padded_batch['ddg'].append(sample['ddg'])#append(torch.tensor(float(sample['ddg']), dtype=torch.float32))  \n",
    "#         #padded_batch['alpha_vec'].append(sample['alpha_vec'])#append(torch.tensor(sample['alpha_vec'], dtype=torch.float32))  \n",
    "#         #padded_batch['pos_mut'].append(sample['pos_mut'])#append(torch.tensor(sample['pos_mut'], dtype=torch.int64))  \n",
    "\n",
    "#     # Convert list of tensors into a single batch tensor\n",
    "#     padded_batch['wild_type'] = torch.stack(padded_batch['wild_type'])  # Shape: (batch_size, max_len, max_features)\n",
    "#     padded_batch['mut_type'] = torch.stack(padded_batch['mut_type'])  \n",
    "#     padded_batch['length'] = torch.stack(padded_batch['length'])  \n",
    "#     padded_batch['ddg'] = torch.stack(padded_batch['ddg'])  \n",
    "#     #padded_batch['alpha_vec'] = torch.stack(padded_batch['alpha_vec'])  \n",
    "#     #padded_batch['pos_mut'] = torch.stack(padded_batch['pos_mut'])  \n",
    "\n",
    "#     return padded_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf636760-6707-4522-b204-9bdadcb29efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "E_TYPE = 'ESM2'\n",
    "dataloader_test = dataloader_generation_pred(E_TYPE,  test_path=['s669_Castrense.pkl'],  batch_size = 6, dataloader_shuffle = False, inv= False)\n",
    "#['s669_Castrense.pkl']#'PTMUL_D.pkl'\n",
    "#'ddg_S2648_ESM2_ALL_LENGTH.pkl' #'s2450_fold_4.pkl' \n",
    "#['../DeltaDelta_BELLO/cdna117k_fold_1.pkl'] + ['../DeltaDelta_BELLO/cdna117k_fold_2.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2869da-2d53-4419-b0df-5417521bf81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def model_performance_test(model, dataloader_test, inv= False, train = False):\n",
    "    # Assicurati che il modello sia in modalità di valutazione\n",
    "    model.eval()\n",
    "    \n",
    "    # Lista per salvare tutte le predizioni\n",
    "    all_predictions_test = []\n",
    "    all_lables_test = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "       \n",
    "        for i, batch in enumerate(dataloader_test):\n",
    "\n",
    "            predictions_test, labels_test=output_model_from_batch(batch, model, device, train=False)\n",
    "\n",
    "            # Aggiungi le predizioni alla lista\n",
    "            all_predictions_test.append(predictions_test)\n",
    "            all_lables_test.append(labels_test)\n",
    "\n",
    "    return all_predictions_test, all_lables_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1677da-b1ba-41cc-968e-557714b358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "input_dim = 1280\n",
    "\n",
    "transf_parameters={'input_dim':1280, 'num_heads':8,\n",
    "                    'dropout_rate':0.,}\n",
    "i=4\n",
    "best_model = torch.load('JanusDDG_300epochs.pth')#(f'JanusDDG_300epochs.pth')\n",
    "#torch.load(f'DDGemb_Cross_4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e296cf-51ce-421c-ab1b-fe6fc12a0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test, all_lables_test = model_performance_test(best_model,dataloader_test,\n",
    "                                                          inv=False, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee06d0-a0e6-4327-b20a-1bbc4ecddcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), -torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(),- torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(),- torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee49360-e73b-4945-8f48-a3dc7addce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(torch.cat(all_predictions_test, dim=0).cpu()[:669],torch.cat(all_predictions_test, dim=0).cpu()[669:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ae410-5d3c-4e99-a754-775fdf4c9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.cat(all_predictions_test, dim=0).cpu()[:669] + torch.cat(all_predictions_test, dim=0).cpu()[669:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682cfa5-0193-46d4-8092-398a900d5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(torch.cat(all_predictions_test, dim=0).cpu()).to_pickle('DDGemb_cross_0_predictions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188b42b-b44c-40c2-b63b-5ddd5742d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('indici_ordinati_s669.pkl').sort_values(by='index_castrense')['DDG'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8fd86-2002-4008-83c4-71e242fb371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indici_ordinati = pd.read_pickle('indici_ordinati_s669.pkl').sort_values(by='index_castrense')['index'].values\n",
    "pythia_s669 = pd.read_csv('../DeltaDeltaG/pythia_s669.csv').iloc[indici_ordinati,:]['Pythia_inv']\n",
    "info_mut = pd.read_csv('../DeltaDeltaG/pythia_s669.csv').iloc[indici_ordinati,:][['wildtype','mutation']]\n",
    "pred_janus= np.array(torch.cat(all_predictions_test, dim=0).cpu())\n",
    "true_ddg = pd.read_pickle('indici_ordinati_s669.pkl').sort_values(by='index_castrense')['DDG'].values#np.array(torch.cat(all_lables_test, dim=0).cpu())\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'wild':info_mut.iloc[:,0].values,\n",
    "    'mut':info_mut.iloc[:,1].values,\n",
    "    'pythia_s669': pythia_s669.values,  # Deve avere la stessa lunghezza di predictions\n",
    "    'predictions': pred_janus,  # Ogni riga avrà un valore scalare o un array se multidimensionale\n",
    "    'true_ddg':true_ddg\n",
    "})\n",
    "df.index = indici_ordinati\n",
    "print('Pearson s669: ', pearsonr(df['predictions'],df['true_ddg']))   \n",
    "\n",
    "\n",
    "indici_ordinati_s461  = pd.read_pickle('indici_ordinati_s669.pkl').dropna(subset='s461_pdb').sort_values(by='index_castrense')['index'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a56c97-8744-4a04-bdc9-be2ce4820a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0dbdd-0575-4644-9e16-54e5365361fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rose_csv = pd.read_csv('rose1985.csv')\n",
    "\n",
    "def rose_score(row):\n",
    "    wild_rose = rose_csv[rose_csv['Parameter']==row['wild']]['Rose1985'].values[0]\n",
    "    mut_rose = rose_csv[rose_csv['Parameter']==row['mut']]['Rose1985'].values[0]\n",
    "    return wild_rose - mut_rose\n",
    "    \n",
    "df['Rose'] = df.apply(rose_score,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f42c7-e87f-476d-9e68-1962b73fd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "lr.fit(df[['pythia_s669','Rose']].values,df['true_ddg'].values)\n",
    "pearsonr(lr.predict(df[['pythia_s669','Rose']].values),df['true_ddg'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17326be-596b-4791-a894-0d1ffb5e188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(df['pythia_s669'].values * 0.11 - df['Rose'].values*0.0088,df['true_ddg'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5c78a-0e44-41b1-a867-993aeb12fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s461 = df.loc[indici_ordinati_s461,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6214df-0511-4eb9-bda5-693e62d6c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pearsonr(df_s461['pythia_s669']*0.11-df_s461['Rose']*0.0088 ,df_s461['true_ddg']))   \n",
    "print(pearsonr(df_s461['predictions'],df_s461['true_ddg']))   \n",
    "print('RMSE: ', root_mean_squared_error(df_s461['pythia_s669'],df_s461['true_ddg']))\n",
    "print('MAE: ', mean_squared_error(df_s461['pythia_s669'],df_s461['true_ddg']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac82868-6850-48b1-8658-ce454ecac9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869cf8c-309d-447e-9fe3-d9ff8aa1f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pearsonr(df_s461['predictions'],df_s461['true_ddg']))   \n",
    "print(pearsonr(df['predictions'],df['true_ddg']))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e10514-d351-4bf0-afae-6504b5aa23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test dirette: {pearsonr(-0.07416111+ pythia_s669.values*0.09+np.array(torch.cat(all_predictions_test, dim=0).cpu()*0.57), torch.cat(all_lables_test, dim=0).cpu())}')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000aaa46-0caf-43fd-b2c6-ac151c5259a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(torch.cat(all_predictions_test, dim=0).cpu()).to_pickle('Janus_s669_fake.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99bb4e-193a-4ec3-ab3a-60ffdcd8432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = torch.cat(all_predictions_test, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4fe39f-7e66-4fb2-a3c0-ccbd739828b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = torch.cat(all_predictions_test, dim=0).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d582d9-99a7-432c-adf4-c35abd6c78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot =(dir-inv) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be3358-4f22-4040-8554-79546503f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec72677-0da4-4bf7-9d6b-5e47a2f2cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics( pd.DataFrame(tot),  pd.DataFrame(tot), pd.DataFrame(torch.cat(all_lables_test, dim=0).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec5b3b-2bcb-4d82-8af0-b0433820646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inv = all_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff7662-74e9-42df-b80a-82cde66052b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dirette = all_predictions_test\n",
    "true = all_lables_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b476662-dd1c-4e3a-ab34-e3af2c2a45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "metrics( pd.DataFrame(torch.cat(pred_dirette, dim=0).cpu()), pd.DataFrame(torch.cat(pred_inv, dim=0).cpu()), pd.DataFrame(torch.cat(true, dim=0).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a9d69-6ba4-43e4-ae60-f92bbeafb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "50+49+53+51+51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdbec9-4b21-4531-ba01-8934204c449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    val_Set = [0,1,2,3,4]\n",
    "    val_Set.remove(i)\n",
    "    print(val_Set)\n",
    "    best_model = torch.load(f'DDGemb_Cross_{i}.pth')\n",
    "    \n",
    "    dataloader_val = dataloader_generation_pred(E_TYPE,  test_path=[f's2450_fold_{i}.pkl']+[f's2450_fold_{i}_inv.pkl'],\n",
    "                                                 batch_size = 6, dataloader_shuffle = False, inv= False)\n",
    "    \n",
    "    all_predictions_val, all_lables_val = model_performance_test(best_model,dataloader_val,\n",
    "                                                              inv=False, train=False)\n",
    "    \n",
    "    \n",
    "    print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_val, dim=0).cpu(), torch.cat(all_lables_val, dim=0).cpu())}')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa390cb-681a-4d5a-82cb-46cc2fa384bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecfb7c-e7f0-459e-b576-5e6b0d9d0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesi = np.array([0.2,0.21,0.2,0.19,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abdfd3-0129-4413-8aa0-bb5e72467c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tot = []\n",
    "\n",
    "for i in range(5):\n",
    "    best_model = torch.load(f'DDGemb_Cross_{i}.pth')\n",
    "    \n",
    "    all_predictions_test, all_lables_test = model_performance_test(best_model,dataloader_test,\n",
    "                                                          inv=False, train=False)\n",
    "    \n",
    "    pred_tot.append(pd.Series(torch.cat(all_predictions_test, dim=0).cpu()))#.to_pickle('DDGemb_Cross_0_predictions_s669.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765fbd4-053c-412f-9190-a991bb5b2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inv = pred_tot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9876e-b6e6-4fba-b529-efc556d6cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = pred_tot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6205aa5-01dc-4431-b1bc-535f0ab9cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3707d1d-d3a7-465a-932d-d50fd9580a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "somma_pesata_dir = sum(w * v for w, v in zip(pesi, pred_tot))\n",
    "\n",
    "print(somma_pesata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274aa870-2e17-4408-8b37-e7dced6d6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492fc24c-9dc2-44c1-bc3a-23c8a675d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "somma_pesata_inv = sum(w * v for w, v in zip(pesi, pred_inv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddc96f-7db1-4cee-b761-8145de71a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pred_tot[0]+pred_tot[1]+pred_tot[2]+pred_tot[3]+pred_tot[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b87d4-0dbb-4384-bb30-20f1711a37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(somma_pesata, torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'Spearmanr test dirette: {spearmanr(somma_pesata, torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(somma_pesata, torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(somma_pesata, torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21931a-b3bf-4369-840f-a054a051eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "somma_pesata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fe318-ef98-40d8-8070-897967820ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "somma_pesata_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e1404-8d4c-4849-abda-7df1c6045fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(pd.DataFrame(somma_pesata_dir),pd.DataFrame(somma_pesata_inv),-pd.DataFrame(torch.cat(all_lables_test, dim=0).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab21d1-dd90-40de-8372-bba1ac3d6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([386,342,187,497,105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2c633-7cea-4235-870c-277482660be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793be0cd-e2d2-4936-914b-d5760adddabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(f'DDGemb_Cross_4.pth')\n",
    "all_predictions_test, all_lables_test = model_performance(best_model,dataloader_test,\n",
    "                                                          dataloader_train=None,dataloader_validation=None,inv=True, train=False)\n",
    "\n",
    "pred_tot.append(pd.Series(torch.cat(all_predictions_test, dim=0).cpu()))#.to_pickle('DDGemb_Cross_0_predictions_s669.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b68981-6975-495f-b097-aebe0a727f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'Spearmanr test dirette: {spearmanr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f84dea-1498-489e-a115-e83651c553ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63ef67-190b-4822-8178-1a0592a753dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed1a72-757b-43a3-8783-29d054d95e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138de61-a8eb-449d-b187-818ba64fe978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295bed0-f130-4813-8eb5-06a966923c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce649183-a266-431c-833a-6d704e6e2a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455a826-dbe4-402e-97f0-c146853a0d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cdf3e-44c1-4a1a-8a90-4eab7b2a147e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a015d-7710-42e8-b972-4fbdf49e4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOOTSTRAPPING\n",
    "\n",
    "results_naive=[]\n",
    "for i in range(1000):\n",
    "    dataloader_test = dataloader_generation_pred(E_TYPE,  test_path='ddg_s669_ESM2_HYDRA_LITE.pkl',  batch_size = 128, dataloader_shuffle = False, inv= False)\n",
    "\n",
    "    all_predictions_test, all_lables_test = model_performance(best_model,\n",
    "                                                                                                                                           dataloader_test,\n",
    "                                                                                                                                           dataloader_train=None,\n",
    "                                                                                                                                           dataloader_validation=None,         \n",
    "                                                                                                                                           inv=False,\n",
    "                                                                                                                                           train=True)\n",
    "    results_naive.append(pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())[0]) \n",
    "\n",
    "pd.DataFrame(results_naive).to_pickle('result_bootstrapping_MLP_27_01_2025.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869eca65-1ecd-4ae1-874e-96e50352be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('MLP_HYDRA_MEAN_29_01_2025.pth')#('MLP_HYDRA_MEAN_29_01_2025.pth')#'MLP_27_01_2025.pth')#('MPL_HYDRA_model_7W_256_256.pth')\n",
    "best_model.hydra=True\n",
    "results_hydra=[]\n",
    "for i in range(1000):\n",
    "    dataloader_test = dataloader_generation_pred(E_TYPE,  test_path='ddg_s669_ESM2_HYDRA_LITE.pkl',  batch_size = 128, dataloader_shuffle = False, inv= False)\n",
    "\n",
    "    all_predictions_test, all_lables_test = model_performance(best_model,\n",
    "                                                                                                                                           dataloader_test,\n",
    "                                                                                                                                           dataloader_train=None,\n",
    "                                                                                                                                           dataloader_validation=None,         \n",
    "                                                                                                                                           inv=False,\n",
    "                                                                                                                                           train=True)\n",
    "    results_hydra.append(pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())[0]) \n",
    "\n",
    "pd.DataFrame(results_hydra).to_pickle('result_bootstrapping_MLP_HYDRA_MEAN_29_01_2025.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b7fb2-6a9a-442a-8f83-332dc5f55ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(results_hydra,kde=True,color='blue')\n",
    "sns.histplot(results_naive,kde=True,color='red')\n",
    "from scipy.stats import ks_2samp\n",
    "ks_2samp(results_hydra,results_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c7802-9ca4-4097-85e7-685cf7901750",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(results_hydra,kde=True)\n",
    "sns.histplot(results_naive,kde=True)\n",
    "from scipy.stats import ks_2samp\n",
    "ks_2samp(results_hydra,results_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424bec2-19c5-4010-a42d-6222c6c20271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'Spearmanr test dirette: {spearmanr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828a81b-024a-414b-8e5e-6ba90ee5075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(torch.cat(all_predictions_test, dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a65ae-51b9-4f60-9316-9af940d97886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(torch.cat(all_predictions_train, dim=0).cpu()).to_pickle('MLP_HYDRA_7W_256_256_predictions_esm2_train.pkl')\n",
    "pd.Series(torch.cat(all_predictions_test, dim=0).cpu()).to_pickle('MLP_HYDRA_MEAN_29_01_2025.pkl')\n",
    "#pd.Series(torch.cat(all_predictions_validation, dim=0).cpu()).to_pickle('MLP_HYDRA_7W_256_256_predictions_esm2_val.pkl')\n",
    "# pd.Series(torch.cat(all_lables_train, dim=0).cpu()).to_pickle('MLP_all_lables_train.pkl')\n",
    "# pd.Series(torch.cat(all_lables_test, dim=0).cpu()).to_pickle('MLP_all_lables_test.pkl')\n",
    "# pd.Series(torch.cat(all_lables_validation, dim=0).cpu()).to_pickle('MLP_all_lables_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e0ef4-d52f-4d60-8d75-e64b0f8e25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####NOMI \n",
    "#'MLP_HYDRA_7W_256_256_predictions_esm2_train.pkl'\n",
    "\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b33ed-c344-46dd-91ae-6055e8523dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(torch.cat(all_predictions_test, dim=0).cpu()).to_pickle('MLP_HYDRA_7W_256_256_predictions_Protherm_Doubles_pdbnum_2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8d7e1-f460-49ca-8210-45949461934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test d-r: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_predictions_test_inv, dim=0).cpu())}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87dcd48-a7fd-473c-b1fe-e87ac8963672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'Spearmanr test dirette: {spearmanr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "\n",
    "\n",
    "# Pearson test dirette: PearsonRResult(statistic=0.49811210357961794, pvalue=3.0676935334080987e-43)\n",
    "# Pearson test dirette: SignificanceResult(statistic=0.5209411003368696, pvalue=8.229743816346383e-48)\n",
    "# RMSE dirette: 1.4306520223617554\n",
    "# MAE dirette: 1.0020039081573486\n",
    "\n",
    "# Pearson test dirette: PearsonRResult(statistic=0.4984754880369162, pvalue=2.6104865839652876e-43)\n",
    "# Pearson test dirette: SignificanceResult(statistic=0.5219817530960136, pvalue=4.996030095808217e-48)\n",
    "# RMSE dirette: 1.4335042238235474\n",
    "# MAE dirette: 1.0032145977020264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b8682-79ad-4a78-b29e-b7abe1e0c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test dirette: {spearmanr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bba584-dbcf-4a89-9d90-7bc9d3aede00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test inverse: {pearsonr(torch.cat(all_predictions_test_inv, dim=0).cpu(), torch.cat(all_lables_test_inv, dim=0).cpu())}')    \n",
    "print(f'RMSE inve: {root_mean_squared_error(torch.cat(all_predictions_test_inv, dim=0).cpu(), torch.cat(all_lables_test_inv, dim=0).cpu())}')\n",
    "print(f'MAE inve: {mean_absolute_error(torch.cat(all_predictions_test_inv, dim=0).cpu(), torch.cat(all_lables_test_inv, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c026492-0b77-4ae9-912c-35632ee628df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test.extend(all_predictions_test_inv)\n",
    "all_lables_test.extend(all_lables_test_inv)\n",
    "print(f'Pearson test dirette+inverse: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette+inverse: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette+inverse: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b1761-2941-44ed-95be-20772e977033",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286b45b-f9f4-4dd2-b2ea-60993c2c1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855b4db-5922-4b5d-a421-1a632f2b591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esm2_train = pd.read_pickle('MLP_all_predictions_esm2_s2648.pkl')\n",
    "y_esm2_test = pd.read_pickle('MLP_all_predictions_esm2_s669.pkl')\n",
    "\n",
    "y_ohe_train = pd.read_pickle('MLP_all_predictions_ohe_s2648.pkl')\n",
    "y_ohe_test = pd.read_pickle('MLP_all_predictions_ohe_s669.pkl')\n",
    "\n",
    "y_true_test = pd.DataFrame(torch.cat(all_lables_test, dim=0).cpu())\n",
    "y_true_train = pd.DataFrame(torch.cat(all_lables_train, dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f120e-4350-4b51-a81f-4af0feafad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train = pd.concat([y_esm2_train,y_ohe_train],axis=1)\n",
    "X_test = pd.concat([y_esm2_test,y_ohe_test],axis=1) \n",
    "y_train = y_true_train\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "prediction_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cacfa9-a935-458c-a693-bc147810073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lr.reshape(669).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3083a-a2a1-4043-9bdc-7a76079cd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a4524-38b8-4eb0-aa56-bff0006459de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assicurati che entrambe le variabili siano monodimensionali\n",
    "y_true_test_flat = y_true_test.values.ravel()  # Oppure .flatten()\n",
    "prediction_lr_flat = prediction_lr.reshape(669).ravel()  # Oppure np.squeeze()\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(y_true_test_flat, prediction_lr_flat)}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(y_true_test_flat, prediction_lr_flat)}')  \n",
    "print(f'MAE dirette: {mean_absolute_error(y_true_test_flat, prediction_lr_flat)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0307e-7a1c-4b38-ab34-4210aa56dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test dirette: {pearsonr(y_true_test_flat, X_test.iloc[:,0])}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(y_true_test_flat, X_test.iloc[:,0])}')  \n",
    "print(f'MAE dirette: {mean_absolute_error(y_true_test_flat, X_test.iloc[:,0])}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610c903-b8b7-48ab-9c84-9d4130b997f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950ae10-31f9-47a1-aaec-de171423eca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c68ce-c0b7-4def-b967-8d3570324d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################FINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ddd4a-880f-4362-98d8-f5fccfbf1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ddg_s2648_ESM2.pkl', 'rb') as f:\n",
    "    dataset_s2648 = pickle.load(f)\n",
    "\n",
    "with open('ddg_s669_ESM2.pkl', 'rb') as f:\n",
    "    dataset_s669 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1ae21-c521-4c46-9470-374043e613d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Creiamo il dataset\n",
    "dataset_train = ProteinDataset(dataset_s2648, threshold=5.0)\n",
    "dataset_test = ProteinDataset(dataset_s669, threshold=5.0)\n",
    "\n",
    "\n",
    "# Creazione di un DataLoader\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1d9c2-7b09-45a3-b47d-669c5c145ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_embedding_aa = {}\n",
    "ordine_aa = {}\n",
    "\n",
    "aa_a = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "ind_pos = 0\n",
    "for name_aa in [*aa_a]:\n",
    "    dic_embedding_aa[name_aa] = np.zeros(20)\n",
    "    dic_embedding_aa[name_aa][ind_pos] = 1\n",
    "\n",
    "    ordine_aa[name_aa] = ind_pos\n",
    "\n",
    "    ind_pos+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84332f-a5c7-4bb1-aede-ccf84b9e7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_letters_test = []\n",
    "info_letters_train = []\n",
    "info_mutation_test = []\n",
    "info_mutation_train = []\n",
    "\n",
    "for batch in dataloader_test:\n",
    "    info_letters_test = []\n",
    "    \n",
    "    for batch_i in range(len(batch['id'])):\n",
    "        vector_w = [1 if i == 1 else 0 for i in batch['V'][batch_i][:]]\n",
    "        vector_m = [1 if i == -1 else 0 for i in batch['V'][batch_i][:]]\n",
    "        letter_w = next((k for k, v in dic_embedding_aa.items() if (v == vector_w).all()), None)\n",
    "        letter_m = next((k for k, v in dic_embedding_aa.items() if (v == vector_m).all()), None)\n",
    "        letters = [letter_w + letter_m]\n",
    "        info_letters_test.extend(letters)\n",
    "\n",
    "    info_mutation_test.extend([id+l[0]+str(pos)+l[-1] for id, pos, l in zip(batch['id'],batch['position'].tolist(), info_letters_test)])\n",
    "    \n",
    "\n",
    "for batch in dataloader_train:\n",
    "    info_letters_train = []\n",
    "    for batch_i in range(len(batch['id'])):\n",
    "        vector_w = [1 if i == 1 else 0 for i in batch['V'][batch_i][:]]\n",
    "        vector_m = [1 if i == -1 else 0 for i in batch['V'][batch_i][:]]\n",
    "        letter_w = next((k for k, v in dic_embedding_aa.items() if (v == vector_w).all()), None)\n",
    "        letter_m = next((k for k, v in dic_embedding_aa.items() if (v == vector_m).all()), None)\n",
    "        letters = [letter_w + letter_m]\n",
    "        info_letters_train.extend(letters)\n",
    "\n",
    "    info_mutation_train.extend([id+l[0]+str(pos)+l[-1] for id, pos, l in zip(batch['id'],batch['position'].tolist(), info_letters_train)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab45e0f-fa6a-4086-935e-3938844ac5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(info_mutation_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c3c7f-3d36-4e11-a170-735f19109641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assicurati che il modello sia in modalità di valutazione\n",
    "DDG_model.eval()\n",
    "\n",
    "# Lista per salvare tutte le predizioni\n",
    "all_predictions_test = []\n",
    "all_predictions_train = []\n",
    "\n",
    "all_lables_train = []\n",
    "all_lables_test = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for i, batch in enumerate(dataloader_test):\n",
    "        graph_x = batch.x.float().to(device)   #wild_type\n",
    "        graph_edge_index = batch.edge_index.to(device)   #wild_type\n",
    "        graph_batch = batch.batch.to(device)   #wild_type\n",
    "        labels = batch.y.float().float().to(device)\n",
    "        position_wild = position_adj(batch.position, batch.ptr)\n",
    "        position_mut = position_mut_adj(batch.position, batch.ptr)\n",
    "        intervallo_wild = edge_index_wild(graph_x, batch.ptr)\n",
    "        intervallo_mut = edge_index_mut(graph_x, batch.ptr)\n",
    "        V = batch.V\n",
    "        edge_weights = batch.edge_weight\n",
    "        predictions = DDG_model(graph_x, graph_edge_index, labels,position_wild,position_mut, V,\n",
    "                        edge_weights = edge_weights,intervallo_wild= intervallo_wild,intervallo_mut =intervallo_mut)  \n",
    "            \n",
    "        all_predictions_test.append(predictions)\n",
    "        all_lables_test.append(labels)\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        graph_x = batch.x.float().to(device)   #wild_type\n",
    "        graph_edge_index = batch.edge_index.to(device)   #wild_type\n",
    "        graph_batch = batch.batch.to(device)   #wild_type\n",
    "        labels = batch.y.float().float().to(device)\n",
    "        position_wild = position_adj(batch.position, batch.ptr)\n",
    "        position_mut = position_mut_adj(batch.position, batch.ptr)\n",
    "        intervallo_wild = edge_index_wild(graph_x, batch.ptr)\n",
    "        intervallo_mut = edge_index_mut(graph_x, batch.ptr)\n",
    "        V = batch.V\n",
    "        edge_weights = batch.edge_weight\n",
    "        predictions = DDG_model(graph_x, graph_edge_index, labels,position_wild,position_mut, V,\n",
    "                        edge_weights = edge_weights,intervallo_wild= intervallo_wild,intervallo_mut =intervallo_mut) \n",
    "\n",
    "        \n",
    "        # Aggiungi le predizioni alla lista\n",
    "        all_predictions_train.append(predictions)\n",
    "        all_lables_train.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fdd0e-7e05-4a49-8c1b-db7b289aabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test = torch.cat([i[0] for i in all_predictions_test], dim=0)\n",
    "all_predictions_train = torch.cat([i[0] for i in all_predictions_train], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf0cb5-633f-4d78-9876-0f7bdae04629",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_train_dir = all_predictions_train[:int(all_predictions_train.shape[0]/2)]\n",
    "all_predictions_train_inv = all_predictions_train[int(all_predictions_train.shape[0]/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f7404-755a-457e-a7da-12cdabf9ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test_dir = all_predictions_test[:int(all_predictions_test.shape[0]/2)]\n",
    "all_predictions_test_inv = all_predictions_test[int(all_predictions_test.shape[0]/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea3b27-42aa-4459-bfd2-442daa99c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(all_predictions_test_dir.cpu(),all_predictions_test_inv.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91955d77-5468-454e-9dd5-0243644751da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(all_predictions_train.cpu(),torch.cat(all_lables_train, dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8279c-1a5a-4acf-9358-f7e7b2386ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_guido_test = pd.DataFrame(np.array([info_mutation_test, all_predictions_test.cpu().numpy()]).T)\n",
    "prediction_guido_train = pd.DataFrame(np.array([info_mutation_train, all_predictions_train.cpu().numpy()]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1eaf7-3939-49e9-a135-182b84877a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_guido_train.drop_duplicates(subset=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45bb2a-f484-415f-9cb0-8e8e7df58621",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_guido_train.iloc[:,1] = prediction_guido_train.iloc[:,1].map(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9603ed6-2d54-4c63-9b1d-13f439cd118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_guido_train.iloc[:,1] = -prediction_guido_train.iloc[:,1]\n",
    "prediction_guido_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1dfe35-a149-4670-893e-80f0b6528db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f229fc-9b26-47fc-9389-2e01966301ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_guido_test.to_pickle('prediction_guido_test.pkl')\n",
    "prediction_guido_train.to_pickle('prediction_guido_train.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3895da-089f-43c5-9c2c-33e3badf6a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../DeltaDeltaG/prediction_guido_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43239c-2d26-4eae-b139-18d5a1897470",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DDG_model.state_dict(), 'model_weights_pyhzia.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b4661-f10c-4b38-b79e-769d6b5a4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9d111-4bfa-4e3e-b486-5f2b42bd4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([float(d['ddg']) for d in dataset_s2648])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88b791-ef3b-4be2-84c9-b18ff69f337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c46748-a8fb-4a2c-93b8-3ac269b3c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(x=[float(d['ddg']) for d in dataset_s669] + all_predictions_test.tolist(),\n",
    "            hue=([\"true\"] * len(dataset_s669) + [\"pred\"] * len(dataset_s669)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27693c02-73ae-4ca2-ad48-2ab665a0e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(x=[float(d['ddg']) for d in dataset_s2648] + prediction_guido_train[1].tolist(),\n",
    "            hue=([\"true\"] * len(dataset_s2648) + [\"pred\"] * len(dataset_s2648)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255bc5e-eeb9-4be3-8849-dd588580165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction_guido_train[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aa8a7-39d6-49e8-9301-5408400d0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot([float(true['ddg']) for true in dataset_s669])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fab0ab-0458-4c09-ae1d-433d23fc8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(prediction_guido_test.iloc[:,1].values.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffe30b-2757-4275-99d6-94ef547f3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "sns.scatterplot(y = prediction_guido_test.iloc[:,1].values.astype(float), x= [float(true['ddg']) for true in dataset_s669])\n",
    "sns.scatterplot(y =  [float(true['ddg']) for true in dataset_s669],x= [float(true['ddg']) for true in dataset_s669])\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Pred')\n",
    "plt.savefig('prova.pdf')\n",
    "\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbbdeb-f8d5-4f73-aeac-669adad74b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(prediction_guido_test.iloc[:,1].values.astype(float),[float(true['ddg']) for true in dataset_s669])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264a83d-b099-4fa5-9e02-3c4f6cec19e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
